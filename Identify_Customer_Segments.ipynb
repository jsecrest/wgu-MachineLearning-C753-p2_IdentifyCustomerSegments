{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JLS: Preemptive notes from student <a name=\"PreemptiveNotes\"></a>\n",
    "\n",
    "\n",
    "## Initials (JLS) signify unique content\n",
    "\n",
    "This notebook contains a lot of supplied notes and comments. I've tried to preface\n",
    "many areas that I've created with JLS. This is a pretty big notebook\n",
    "so hopefully that helps. Not all of my sections contain this annotation.\n",
    "\n",
    "\n",
    "## Python Environment\n",
    "\n",
    "I downloaded this notebook from udacity, but I am using my own development environment.\n",
    "- The environment for this notebook is **Python 3.10.4**\n",
    "- For module versions see \n",
    "[JLS: Python Development Environment](#DevEnv) at the end of this notebook.\n",
    "- See also: [requirements.txt](requirements.txt)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Identify Customer Segments\n",
    "\n",
    "In this project, you will apply unsupervised learning techniques to identify segments of the population that form the core customer base for a mail-order sales company in Germany. These segments can then be used to direct marketing campaigns towards audiences that will have the highest expected rate of returns. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "This notebook will help you complete this task by providing a framework within which you will perform your analysis steps. In each step of the project, you will see some text describing the subtask that you will perform, followed by one or more code cells for you to complete your work. **Feel free to add additional code and markdown cells as you go along so that you can explore everything in precise chunks.** The code cells provided in the base template will outline only the major tasks, and will usually not be enough to cover all of the minor tasks that comprise it.\n",
    "\n",
    "It should be noted that while there will be precise guidelines on how you should handle certain tasks in the project, there will also be places where an exact specification is not provided. **There will be times in the project where you will need to make and justify your own decisions on how to treat the data.** These are places where there may not be only one way to handle the data. In real-life tasks, there may be many valid ways to approach an analysis task. One of the most important things you can do is clearly document your approach so that other scientists can understand the decisions you've made.\n",
    "\n",
    "At the end of most sections, there will be a Markdown cell labeled **Discussion**. In these cells, you will report your findings for the completed section, as well as document the decisions that you made in your approach to each subtask. **Your project will be evaluated not just on the code used to complete the tasks outlined, but also your communication about your observations and conclusions at each stage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must remember to run before later free_life() !\n",
    "\n",
    "# import libraries here; add more as necessary\n",
    "## general python\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import random\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "## basic data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#JLS\n",
    "import compile_data_dictionary as cdd\n",
    "\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#JLS: I'm choosing to use an updated environment, rending the below statement obsolete\n",
    "# '''\n",
    "# Import note: The classroom currently uses sklearn version 0.19.\n",
    "# If you need to use an imputer, it is available in sklearn.preprocessing.Imputer,\n",
    "# instead of sklearn.impute as in newer versions of sklearn.\n",
    "# '''\n",
    "\n",
    "random.seed(444)\n",
    "\n",
    "#JLS Custom typing\n",
    "DataFrame = type(pd.DataFrame())\n",
    "Series = type(pd.Series(dtype=\"object\"))\n",
    "\n",
    "# for helping select a maplotlib color style\n",
    "# matplotlib.style.available\n",
    "# matplotlib.style.use('default')\n",
    "matplotlib.style.use('tableau-colorblind10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load the Data\n",
    "\n",
    "There are four files associated with this project (not including this one):\n",
    "\n",
    "- `Udacity_AZDIAS_Subset.csv`: Demographics data for the general population of Germany; 891211 persons (rows) x 85 features (columns).\n",
    "- `Udacity_CUSTOMERS_Subset.csv`: Demographics data for customers of a mail-order company; 191652 persons (rows) x 85 features (columns).\n",
    "- `Data_Dictionary.md`: Detailed information file about the features in the provided datasets.\n",
    "- `AZDIAS_Feature_Summary.csv`: Summary of feature attributes for demographics data; 85 features (rows) x 4 columns\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. You will use this information to cluster the general population into groups with similar demographic properties. Then, you will see how the people in the customers dataset fit into those created clusters. The hope here is that certain clusters are over-represented in the customers data, as compared to the general population; those over-represented clusters will be assumed to be part of the core user base. This information can then be used for further applications, such as targeting for a marketing campaign.\n",
    "\n",
    "To start off with, load in the demographics data for the general population into a pandas DataFrame, and do the same for the feature attributes summary. Note for all of the `.csv` data files in this project: they're semicolon (`;`) delimited, so you'll need an additional argument in your [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call to read in the data properly. Also, considering the size of the main dataset, it may take some time for it to load completely.\n",
    "\n",
    "Once the dataset is loaded, it's recommended that you take a little bit of time just browsing the general structure of the dataset and feature summary file. You'll be getting deep into the innards of the cleaning in the first major step of the project, so gaining some general familiarity can help you get your bearings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the general demographics data.\n",
    "azdias = pd.read_csv(\"data/Udacity_AZDIAS_Subset.csv\", sep=\";\", dtype=\"str\")\n",
    "\n",
    "# Load in the feature summary file.\n",
    "# feat_info = pd.read_csv(\"data/AZDIAS_Feature_Summary.csv\", sep=\";\", dtype=\"str\")\n",
    "# jls note: I've worked with the feature info data in a different way. See Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the data after it's loaded (e.g. print the number of\n",
    "# rows and columns, print the first few rows).\n",
    "\n",
    "def quick_look(df:pd.DataFrame, name:str = \"unlabeled\"):\n",
    "    display(f\"Quick look at {name} dataframe:\")\n",
    "    display(f\"Shape: {df.shape}\")\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_look(azdias, \"azdias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip**: Add additional cells to keep everything in reasonably-sized chunks! Keyboard shortcut `esc --> a` (press escape to enter command mode, then press the 'A' key) adds a new cell before the active cell, and `esc --> b` adds a new cell after the active cell. If you need to convert an active cell to a markdown cell, use `esc --> m` and to convert to a code cell, use `esc --> y`. \n",
    "\n",
    "## Step 1: Preprocessing\n",
    "\n",
    "### Step 1.1: Assess Missing Data\n",
    "\n",
    "The feature summary file contains a summary of properties for each demographics data column. You will use this file to help you make cleaning decisions during this stage of the project. First of all, you should assess the demographics data in terms of missing data. Pay attention to the following points as you perform your analysis, and take notes on what you observe. Make sure that you fill in the **Discussion** cell with your findings and decisions at the end of each step that has one!\n",
    "\n",
    "#### Step 1.1.1: Convert Missing Value Codes to NaNs\n",
    "The fourth column of the feature attributes summary (loaded in above as `feat_info`) documents the codes from the data dictionary that indicate missing or unknown data. While the file encodes this as a list (e.g. `[-1,0]`), this will get read in as a string object. You'll need to do a little bit of parsing to make use of it to identify and clean the data. Convert data that matches a 'missing' or 'unknown' value code into a numpy NaN value. You might want to see how much data takes on a 'missing' or 'unknown' code, and how much data is naturally missing, as a point of interest.\n",
    "\n",
    "**As one more reminder, you are encouraged to add additional cells to break up your analysis into manageable chunks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Compiling the Data Dictionary\n",
    "I got irritated with looking at only missing values. I also wanted to try to understand unexpected values.\n",
    "\n",
    "I probably went overboard and spent way too much time on this but...\n",
    "\n",
    "Behold! `compile_data_dictionary.py`! It compiles all the info in `data/AZDIAS_Feature_Summary.csv`\n",
    "and `data/Data_Dictionary.md` the feature summary. \n",
    "(If you look at the code it's mainly a ton of regex and some data structure arrangements)\n",
    "\n",
    "Now we can look at everything we expect to see in a feature using the DataCodex class from my little module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JLS - use my new module\n",
    "codex = cdd.DataCodex(\n",
    "    data_dict_file=\"data/Data_Dictionary.md\",\n",
    "    feat_summary_file=\"data/AZDIAS_Feature_Summary.csv\",\n",
    ")\n",
    "\n",
    "codex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "# target_feature_name = random.choice(codex.feature_names)\n",
    "target_feature_name = \"ALTERSKATEGORIE_GROB\"\n",
    "codex.display_feature(target_feature_name)\n",
    "\n",
    "# print(codex.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Looking at features that have less definition \n",
    "I want to see how many features don't have clear constraints \n",
    "(encoding hasn't been provided for either `allowed_values` or `missing_or_unknown`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_defined_df = codex.all_df.loc[(codex.all_df[\"allowed_values\"].isnull()) | (codex.all_df[\"missing_or_unknown\"].isnull()),[\"allowed_values\",\"missing_or_unknown\"]]\n",
    "display(under_defined_df)\n",
    "len(under_defined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Review data quality\n",
    "\n",
    "I want to understand, per feature \n",
    "- `actual_missing`: how much data was actually never entered\n",
    "- `nan_codes`: how much data is \"encoded\" as missing\n",
    "- `valid_values`: how much data conforms to known valid encodings\n",
    "- `other_values`: everything else\n",
    "  - in some cases this might be invalid data like human error\n",
    "  - in some cases this might be valid but unquantified data (e.g. not every year has been individually validated)\n",
    "\n",
    "Let's write a function to comprehensively check each feature when it's presented as a Series. We can apply that over \n",
    "our main dataframe to get a full picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_validity_sort(feature_data_s: Series, for_apply = True, cust_codex = None) -> Series:\n",
    "    \"\"\"return a dataframe for the feature with aggregated sums about the data quality\n",
    "    - actual_missing = blanks\n",
    "    - nan_codes = values that have an entry in missing_or_unknown\n",
    "    - valid_values = values that have an entry in allowed_values \n",
    "       and are not in missing_or_unknown (if there are no allowed values, all values are allowed)\n",
    "    - other_values = all other values\n",
    "    \"\"\"\n",
    "    # oof. so somewhere along the line I edited my original codex, and so this function \n",
    "    # doesn't work with the later customer data. Lets ensure it's clean here.\n",
    "    codex = cdd.DataCodex(\n",
    "        data_dict_file=\"data/Data_Dictionary.md\",\n",
    "        feat_summary_file=\"data/AZDIAS_Feature_Summary.csv\",\n",
    "    )\n",
    "\n",
    "    feature_name = feature_data_s.name\n",
    "    if codex.is_feature_in_data(feature_name) == False:\n",
    "        print(f\"Warning! feature_name {feature_name} does not appear in the\" \n",
    "        + \"data_dictionary! Result for this feature will be np.nan\")\n",
    "        return np.nan   \n",
    "    feature_meta_s = codex.get_feature_as_s(feature_name)\n",
    "    # feature_data_s = azdias.loc[:,feature_name]  \n",
    "\n",
    "    value_counts = feature_data_s.value_counts()\n",
    "    value_list = value_counts.index.to_series()\n",
    "\n",
    "    # ## Count ACTUAL MISSING values\n",
    "    # represent as a pd.Series for consistency in output\n",
    "    natural_missing_count = pd.Series(data=[len(feature_data_s) - sum(value_counts)],name=feature_name, dtype=\"int64\")\n",
    "\n",
    "    #Count NAN CODES\n",
    "    missing_or_unknown = feature_meta_s.loc[\"missing_or_unknown\"]\n",
    "    if missing_or_unknown is None:\n",
    "        nan_code_counts = pd.Series(dtype=\"int64\")\n",
    "    else:\n",
    "        nan_code_counts = value_counts.loc[value_list.isin(missing_or_unknown)]\n",
    "        #dropping nan codes from value_counts prevents counting them in \"other_values\"\n",
    "        value_counts.drop(value_list.loc[value_list.isin(missing_or_unknown)], inplace=True)\n",
    "        value_list = value_list.loc[~value_list.isin(missing_or_unknown)]\n",
    "\n",
    "    #Count VALID and INVALID\n",
    "    allowed_values = feature_meta_s.loc[\"allowed_values\"]\n",
    "    if allowed_values is None:\n",
    "        #if there isn't anything to compare against, assume every value is valid\n",
    "        valid_value_counts = value_counts\n",
    "        invalid_value_counts = pd.Series(dtype=\"int64\")\n",
    "    else:\n",
    "        valid_value_counts = value_counts.loc[value_list.isin(allowed_values)]\n",
    "        invalid_value_counts = value_counts.loc[~value_list.isin(allowed_values)]\n",
    "\n",
    "    counts = [natural_missing_count, nan_code_counts, valid_value_counts, invalid_value_counts]\n",
    "    names = [\"actual_missing\", \"nan_codes\", \"valid_values\", \"other_values\"]\n",
    "    sums = []\n",
    "\n",
    "    for i in range(len(counts)):\n",
    "        sums.append(sum(counts[i]))\n",
    "        # print(sums)\n",
    "\n",
    "    sums_s = pd.Series(sums, index=names, name=\"count\", dtype=\"int64\")\n",
    "    sums_s =  pd.concat([sums_s, pd.Series(sums_s.agg(\"sum\"), index=[\"sum\"])])\n",
    "\n",
    "    if for_apply:\n",
    "        return sums_s\n",
    "        \n",
    "    # mostly for troubleshooting, and exploration - breaks \"apply\"\n",
    "    for i in range(len(counts)):\n",
    "        print(f\"{names[i]}:\\n{counts[i]}\\n\")\n",
    "    #\n",
    "    return pd.DataFrame(data=[sums_s]).T\n",
    "\n",
    "\n",
    "# feature_name = random.choice(codex.feature_names)\n",
    "# feature_name = \"ANZ_HH_TITEL\"\n",
    "feature_name = \"ALTERSKATEGORIE_GROB\"\n",
    "print(f\"Feature Sample: {feature_name}\")\n",
    "value_validity_sort(azdias.loc[:,feature_name], for_apply=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Check the quality of the dataframe\n",
    "Great! Let's check out the quality of our main dataframe. And while we're at it lets make it \n",
    "convenient to do this again in the future for any dataframe of features that's sliced from azdias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to see everything, and notebooks love to cut out the middle.\n",
    "def review_quality(azdias_derived_df = azdias, per_display = 25):\n",
    "    \"\"\"check the quality of the azdias dataframe or a set of it's features\"\"\"\n",
    "    quality_df = azdias_derived_df.apply(value_validity_sort).T\n",
    "    for i in range(len(quality_df)):\n",
    "        if i % per_display == 0:\n",
    "            display(quality_df[i:i+per_display-1])\n",
    "\n",
    "review_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Looking at \"other\" values\n",
    "Other values (not known to be either nan or valid) are the biggest question about how clean the data is.\n",
    "Luckily it looks like not a lot of columns have that property. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_df = azdias.apply(value_validity_sort).T\n",
    "has_other_df = quality_df[quality_df[\"other_values\"] > 0]\n",
    "display(has_other_df)\n",
    "has_other_list = has_other_df.index.to_list()\n",
    "display(has_other_list)\n",
    "for feature_name in has_other_list:\n",
    "    feature_meta_s = codex.get_feature_as_df(feature_name)\n",
    "    display(feature_meta_s.loc[[\"definition\"]])\n",
    "    feature_data_s = azdias.loc[:,feature_name]\n",
    "    display(feature_data_s.value_counts().head())\n",
    "    del feature_data_s\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Clean up columns with \"other\"\n",
    "Two of these columns are years, and the other is a non-zero integer. That should be reasonable to clean up.\n",
    "\n",
    "Years first.\n",
    "\n",
    "This data base is looking at recent years, so we'll be generous and say that anything after 1600 is valid. The current year is 2022, and I'm pretty sure all of our data was gathered before that. Since this data is proprietary and I won't get to look at any refreshed values, I'm not going to worry about being dynamic. Being loose with these dates does allow for more possible error with typos, but I think it's the fairest way to catch issues for these years without understanding more about acceptable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = azdias.loc[:,[\"GEBURTSJAHR\", \"MIN_GEBAEUDEJAHR\"]]\n",
    "\n",
    "#validate year values and convert \"missing\" or invalid years to nan\n",
    "def flag_year(year:str):\n",
    "    #assumption year is a str or can be cast to one\n",
    "    #if any check fails the string is not a year\n",
    "    invalid_replace = np.nan\n",
    "    if str(year).isdigit() is False:\n",
    "        #do first to be sure str can be cast to int\n",
    "        #deals with nan as well as letters\n",
    "        if np.isnan(year) is False:\n",
    "            print(year) #### I want to know if there are any weird entries\n",
    "        return invalid_replace\n",
    "    if int(year) == 0:\n",
    "        #zero would be covered by <1600, but I want to print anything odd that isn't zero\n",
    "        return invalid_replace \n",
    "    if (int(year) < 1600) or (int(year) > 2022):\n",
    "        print(year)  #### I want to know if there are any weird entries\n",
    "        return invalid_replace\n",
    "    return year\n",
    "print(\"Year values before\")\n",
    "review_quality(year_df)\n",
    "year_df = year_df.applymap(flag_year)\n",
    "print(\"Year values after\")\n",
    "review_quality(year_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JLS - That should do it for years! It looks like all of the \"other\" values were legit years, since we didn't change those values at all. We've moved nan values into actual missing by changing them from encoded values to nan. We eventually want to do this for all of our features anyway.\n",
    "\n",
    "Lets apply the change back to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in year_df.columns:\n",
    "    azdias.loc[:,col] = year_df.loc[:,col]\n",
    "del year_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JLS - Lets take a look at these numbers. We read them in as strings, but there are also nan values, and we're not sure what else. If we cast every value to a string (because, for example, nan is not a string), we can use str.isdigit() to make sure it's a digit. We know that 0 is our missing value range, so we'll covert every legit digit back into an int but send any 0's to nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_s = azdias.loc[:,\"ANZ_HAUSHALTE_AKTIV\"]\n",
    "\n",
    "#validate numerical values\n",
    "def check_number(x):\n",
    "    x_as_s = str(x)\n",
    "    if x_as_s.isdigit():\n",
    "        #0 is a \"missing value\" - replace it with nan\n",
    "        if int(x_as_s) != 0:\n",
    "            return x\n",
    "    return np.nan\n",
    "\n",
    "# number_s = azdias.loc[:,\"ANZ_HAUSHALTE_AKTIV\"]\n",
    "# number_s is a series so we can't use our df oriented quality function\n",
    "display(value_validity_sort(number_s).to_frame().T) \n",
    "number_s = number_s.map(check_number)\n",
    "display(value_validity_sort(number_s).to_frame().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JLS - Again, this looks good! The values entered all seem valid, so we'll just accept the change in our nan_codes and move on!\n",
    "\n",
    "Let's not forget to save these changes to our main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.loc[:,\"ANZ_HAUSHALTE_AKTIV\"] = number_s\n",
    "del number_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Correcting remaining NaN codes\n",
    "We've dealt with all of our columns that had unknown \"other\" values. They still count as \"other\" but we've run validations on all of them.\n",
    "\n",
    "That means that all of our data is clean! We just want to make it consistent now. Everything that was recorded as a \"missing or unknown\" value can all just be converted to nan values.\n",
    "\n",
    "Let's start by looking at our data quality again, and getting a list of every feature that contains nan encoding. We won't expect to see anything that has \"other\" values, since we've already converted those nan_code values to actual_missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_df = azdias.apply(value_validity_sort).T\n",
    "has_nan_df = quality_df.loc[quality_df[\"nan_codes\"] > 0]\n",
    "display(has_nan_df)\n",
    "print(len(has_nan_df))\n",
    "has_nan_list = has_nan_df.index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! One nice side effect of our previous cleanup is that every remaining feature containing nan_codes *also* has a well \n",
    "defined list of allowed values. That means it will be super easy to clean these up using pd.Series.map and a dictionary\n",
    "that \"maps\" allowed values back to themselves.\n",
    "\n",
    "That doesn't sound very useful on the surface, but the awesome thing about using a mapping dictionary is that the series \n",
    "map function will convert everything ***outside*** of that dictionary to nan values for us! We can also ask it to ignore\n",
    "existing nan values for faster processing!\n",
    "\n",
    "This mapping functionality with a dictionary appears to be unique to the \n",
    "[Series.map](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html) function. \n",
    "[Dataframe.applymap](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.applymap.html)\n",
    "doesn't accept a dictionary - it will only take a function for modifying each value in the dataframe. Besides that,\n",
    "we need to use the series to grab our list of allowed values from the codex, per feature.\n",
    "\n",
    "TLDR: We need to write a function that can be applied to the dataframe. \n",
    "Then we can map the passed feature series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_to_nan(feature_data_s:Series) -> Series:\n",
    "    \"\"\"convert all remaining \"missing\" encoded values in a series to actual nan values.\"\"\"\n",
    "    clean_codex = cdd.DataCodex(\n",
    "            data_dict_file=\"data/Data_Dictionary.md\",\n",
    "            feat_summary_file=\"data/AZDIAS_Feature_Summary.csv\",\n",
    "        )\n",
    "    feature_name = feature_data_s.name\n",
    "    allowed_values = set(clean_codex.get_feature_as_s(feature_name).loc[\"allowed_values\"])\n",
    "    allowed_dict = dict([(k,k) for k in allowed_values])\n",
    "    return feature_data_s.map(allowed_dict, na_action=\"ignore\")\n",
    "\n",
    "def apply_missing_to_nan(feature_name_list:List[str], inspect:int=None, cust_df=pd.DataFrame()) -> DataFrame:\n",
    "    \"\"\"convenience function for applying the above function to identified features.\n",
    "    optionally prints before and after comparisons using the tail() method.\"\"\"\n",
    "    if cust_df.empty:\n",
    "        nan_encoded_df = azdias.loc[:,feature_names]\n",
    "    else:\n",
    "        nan_encoded_df = cust_df.loc[:,feature_names]      \n",
    "    if inspect:\n",
    "        display(nan_encoded_df.apply(value_validity_sort).T.tail(inspect))\n",
    "    nan_encoded_df = nan_encoded_df.apply(missing_to_nan)\n",
    "    if inspect:\n",
    "        display(nan_encoded_df.apply(value_validity_sort).T.tail(inspect))\n",
    "    return nan_encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to do a quick \"litmus test\" before I run this on the big list.\n",
    "I've randomly chosen two of the features from our list to give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"AGER_TYP\", \"ALTER_HH\"]\n",
    "apply_missing_to_nan(feature_names, inspect=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great! Let's apply this to the full list of nan_encoded features and save those changes to the main dataframe. I'm feeling pretty confident so I just want to spot check the last half of the list, as it had a good variety of nan_code counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = has_nan_list\n",
    "azdias.loc[:,feature_names]=apply_missing_to_nan(feature_names, inspect=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "\n",
    "Let's just take one last look at everything to make sure we haven't missed anything obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_quality(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1.2: Assess Missing Data in Each Column\n",
    "\n",
    "How much missing data is present in each column? There are a few columns that are outliers in terms of the proportion of values that are missing. You will want to use matplotlib's [`hist()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html) function to visualize the distribution of missing value counts to find these columns. Identify and document these columns. While some of these columns might have justifications for keeping or re-encoding the data, for this project you should just remove them from the dataframe. (Feel free to make remarks about these outlier columns in the discussion, however!)\n",
    "\n",
    "For the remaining features, are there any patterns in which columns have, or share, missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an assessment of how much missing data there is in each column of the\n",
    "# dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Maybe not standard histograms....\n",
    "# NOTE TO GRADER: Feedback welcome on histogram use\n",
    "This dataset ***looks*** like numbers but\n",
    "it's actually text - most of the numbers are actually encodings for non-number concepts. \n",
    "\n",
    "For example, you wouldn't add years together to understand how many years of data are in this database. \n",
    "And you wouldn't divide a building type by its location in east germany. That nonsense that can be easy to do when you \n",
    "treat string data as number data.\n",
    "\n",
    "We even have a few encodings that require text (see my quick check for these types of fields below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_digit_series(allowed_set:set):\n",
    "    def get_non_digits(x):\n",
    "        return str(x).isdigit() == False\n",
    "    # [get_non_digits(x) for x in allowed_set if allowed_set]\n",
    "    if allowed_set:\n",
    "        if len(allowed_set) > 0:\n",
    "            return(sum([get_non_digits(x) for x in allowed_set]))\n",
    "        return None\n",
    "\n",
    "\n",
    "digit_check_s = codex.all_df.allowed_values.apply(find_non_digit_series)\n",
    "codex.all_df.allowed_values.loc[(digit_check_s > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've read in the entire database ***as a string*** to help keep me realistic about how I treat the data. Then I converted a lot of values\n",
    "to pd.nan (a float value) to represent the missing data. So if I have both data and missing data in the same feature, the related series h\n",
    "as both strings and floats in it.\n",
    "\n",
    "Histogram tools appear to hate mixing strings with floats.\n",
    "\n",
    "Not to worry though! Realy, a histogram is a sorted barchart of summed values. I have summed values! Remember our `value_validity_sort` function, that we used for looking at missing values originally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = azdias.apply(value_validity_sort).T\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the data we need! But we don't really need everything... `nan_codes` is meaningless now, and it's not helpful to \n",
    "plot the total number of rows, so we won't need `sum`.\n",
    "\n",
    "We'll sort by `actual_missing`, and we'll take a wholistic look at each feature by plotting it's \"valid\" and \"other\" \n",
    "values along with the missing. Keep in mind that in the context of this graph \"valid\" just means a value was in an\n",
    "expected list. We've done some loose validation on \"other\", so we think it's good data. It's just not constrained to a specific list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_missing(df:DataFrame):\n",
    "    counts = df.apply(value_validity_sort).drop([\"nan_codes\", \"sum\"]).T\n",
    "    counts = counts.sort_values(\"actual_missing\", ascending=False)\n",
    "    counts.plot.bar(stacked=True, figsize=(20,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_missing(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Some of these columns have huge holes. I want a more precise look at our top 10 to understand exactly what percent of the data is missing. I'm going to normalize the values this time to make it easier to see that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.preprocessing.Normalizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer(norm=\"max\")\n",
    "counts = azdias.apply(value_validity_sort).drop([\"nan_codes\"]).T\n",
    "counts = counts.sort_values(\"actual_missing\", ascending=False)\n",
    "counts_norm = pd.DataFrame(norm.fit_transform(counts), index=counts.index, columns=counts.columns)\n",
    "# By normalizing with \"sum\" included we allow it to be the \"max\" value, to which the other values are scaled\n",
    "# This makes all the bars stack to \"1.0\" which is prettier to graph and makes more sense to read when we're talking\n",
    "# about a \"%\" of missing data.\n",
    "\n",
    "# Keeping \"sum\" in the data makes for a funny graph. Get rid of it again, and then graph the top 10 missing columns\n",
    "counts_norm = counts_norm.drop([\"sum\"], axis=1)\n",
    "display(counts_norm.iloc[0:10].plot.bar(stacked=True, figsize=(20,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I need to make a tough call. How much missing data is \"too much\" missing data? \n",
    "There's a really clear drop between ALTER_HH and KKK... And alter is still missing more than a 3rd of it's values, so\n",
    "I'm tempted to make the cut there. Before I do, I want to get an idea of what exactly I'm cutting. Lets look at our\n",
    "data dictionary definitions.\n",
    "\n",
    "I think I'm likely to want to do this a lot, and I didn't include a more limited display function in my codex module,\n",
    "so I'll toss in a new function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_definition_and_allowed_codes(feature_name, len_limit = None) -> None:\n",
    "    \"\"\"display the feature definition and its list of coded allowed values\n",
    "    (length of code list can be limited with `len_limit` if desired)\"\"\"\n",
    "    feature_s = codex.get_feature_as_s(feature_name)\n",
    "    codes = feature_s.loc[\"codes\"]\n",
    "    codes = pd.DataFrame(codes, columns = [\"codes\"])\n",
    "    codes = codes.loc[codes.index.isin(feature_s.allowed_values)]\n",
    "    display(f\"{feature_name}: {feature_s.loc['definition']}\")\n",
    "    if len_limit:\n",
    "        display(codes.iloc[0:len_limit])\n",
    "        if len_limit < len(codes):\n",
    "            display(f\"(showing the first {len_limit} of {len(codes)} codes)\")\n",
    "    else:\n",
    "        display(codes)\n",
    "\n",
    "missing_value_feature_names = counts_norm.iloc[0:6].index\n",
    "for feature_name in missing_value_feature_names:\n",
    "    display_definition_and_allowed_codes(feature_name, len_limit = 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to see how each of these values could contribute to understanding buyer behavior, and I could see how it could be possible to use them in other projects. For example, \"KK_KUNDENTYP\" tracks buyer behavior and it could be interesting to use that value to train a supervised model to predict customer behavior.\n",
    "\n",
    "None of these values seem exceptionally critical so we'll cut all of them off to make sure we're working with the most complete dataset possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"azdias old column length: {len(azdias.columns)}\")\n",
    "azdias = azdias.drop(missing_value_feature_names, axis=1)\n",
    "#quick check to make sure we edited our main DF the way we intended\n",
    "display(f\"azdias new column length: {len(azdias.columns)}\")\n",
    "display(f\"azdias row length {len(azdias)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#display new missing values barchart\n",
    "graph_missing(azdias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those few columns out of the way it's easier to think about the remaining columns with missing values.\n",
    "\n",
    "I see that a lot of columns with similar prefixes have similar amounts of missing data. E.g. KBA05 columns are all very close to each other in the amount of data they are missing, so are the CAMEO columns and the PLZ8 columns. Similarly the FINANZ columns are all very complete, and so are the SEMIO columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion 1.1.2: Assess Missing Data in Each Column\n",
    "\n",
    "Quick summary of the observations I made above:\n",
    "- After sorting columns in descending order by missing values, it's easy to see that there was a clear drop to similar \n",
    "  amounts of missing data after the first 6 columns. I took a quick look at their data dictionary definitions and then cut them. They are listed below in order of the most missing data. See the table below.\n",
    "\n",
    "- I see that a pattern with the missing data is that columns with similar prefixes often have similar amounts of missing data. Not all columns follow this pattern.\n",
    "\n",
    "  |   |Removed Feature| Feature Definition                                 |\n",
    "  |---|---------------|----------------------------------------------------| \n",
    "  | 1 | TITEL_KZ      | Academic title flag                                |\n",
    "  | 2 | AGER_TYP      | Best-ager typology                                 |\n",
    "  | 3 | KK_KUNDENTYP  | Consumer pattern over past 12 months               |\n",
    "  | 4 | KBA05_BAUMAX  | Most common building type within the microcell     |\n",
    "  | 5 | GEBURTSJAHR   | Year of birth                                      |\n",
    "  | 6 | ALTER_HH      | Birthdate of head of household                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1.3: Assess Missing Data in Each Row\n",
    "\n",
    "Now, you'll perform a similar assessment for the rows of the dataset. How much data is missing in each row? As with the columns, you should see some groups of points that have a very different numbers of missing values. Divide the data into two subsets: one for data points that are above some threshold for missing values, and a second subset for points below that threshold.\n",
    "\n",
    "In order to know what to do with the outlier rows, we should see if the distribution of data values on columns that are not missing data (or are missing very little data) are similar or different between the two groups. Select at least five of these columns and compare the distribution of values.\n",
    "- You can use seaborn's [`countplot()`](https://seaborn.pydata.org/generated/seaborn.countplot.html) function to create a bar chart of code frequencies and matplotlib's [`subplot()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html) function to put bar charts for the two subplots side by side.\n",
    "- To reduce repeated code, you might want to write a function that can perform this comparison, taking as one of its arguments a column to be compared.\n",
    "\n",
    "Depending on what you observe in your comparison, this will have implications on how you approach your conclusions later in the analysis. If the distributions of non-missing features look similar between the data with many missing values and the data with few or no missing values, then we could argue that simply dropping those points from the analysis won't present a major issue. On the other hand, if the data with many missing values looks very different from the data with few or no missing values, then we should make a note on those data as special. We'll revisit these data later on. **Either way, you should continue your analysis for now using just the subset of the data with few or no missing values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS Transpose and count\n",
    "An easy way to see how much data is missing is to transpose the dataframe and sum the missing values.\n",
    "Then we can use the describe function on those sums to get a good indication of how many features are missing \n",
    "from how many rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data is missing in each row of the dataset?\n",
    "rows_by_missing_s = azdias.T.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_by_missing_s.name = \"missing_features\"\n",
    "rows_by_missing_s.index.name = \"row_index\"\n",
    "\n",
    "print(\"Row Quality:\")\n",
    "print(\"all rows:\")\n",
    "display(rows_by_missing_s.describe())\n",
    "print(\"Rows missing >= 3 features (top 25% of missing):\")\n",
    "display(rows_by_missing_s.loc[rows_by_missing_s >= 3].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really encouraging! \n",
    "Somewhere between 50% and 75% of our rows don't have any missing data. \n",
    "75% of our rows are missing up to 3 rows.\n",
    "\n",
    "In that top 25% it's still a slow progression toward the max amount of loss.\n",
    "\n",
    "Let's look at this a little more closely and get a visualization of what's happening in that top 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rows_by_missing_s.head())\n",
    "len(rows_by_missing_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_thresholds(threshold, df_subsection, print_progress=True):\n",
    "    start = datetime.now()\n",
    "    # intact_rows = df_subsection.loc[df_subsection <= threshold]\n",
    "    # intact_rows.name = \"intact_rows\"\n",
    "    incomplete_rows = df_subsection.loc[df_subsection > threshold]\n",
    "    incomplete_rows.name = \"incomplete_rows\"\n",
    "    total = len(azdias)\n",
    "    incomplete = len(incomplete_rows)\n",
    "    intact = total - incomplete\n",
    "    feature_loss = round(threshold/len(azdias.T)*100,2)\n",
    "    tradeoff_s = pd.Series(\n",
    "        [intact, \n",
    "        incomplete, \n",
    "        round(intact/total*100,2), \n",
    "        round(incomplete/total*100,2), \n",
    "        feature_loss],\n",
    "        index = [\"intact\",\n",
    "        \"incomplete\",\n",
    "        \"intact_pct\",\n",
    "        \"incomplete_pct\",\n",
    "        \"max_feature_loss_pct\"],\n",
    "        name = f\"{threshold}\"\n",
    "        )\n",
    "    end = datetime.now()\n",
    "    if print_progress:\n",
    "        print(f\"processing threshold: {threshold:<2}...\")\n",
    "        print((end-start))\n",
    "    return tradeoff_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeoff_df = pd.DataFrame([check_thresholds(i, rows_by_missing_s, \\\n",
    "    #I used to run this on every point, but it's taking well over a minute, so I'm changing the \"step from 1 to 2\"\n",
    "    print_progress=False) for i in range(0,50,2)]);\n",
    "\n",
    "tradeoff_df.index.name=\"missing value threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tradeoff_df.iloc[2:9])\n",
    "tradeoff_df.loc[:,[\"intact_pct\", \"incomplete_pct\", \"max_feature_loss_pct\"]].plot(\n",
    "    kind='line',\n",
    "    title='Completeness thresholds',\n",
    "    grid=True,\n",
    "    figsize=(10,6),\n",
    "    markevery=2,\n",
    "    markerfacecolor =\"black\",\n",
    "    ms = 3, \n",
    "    marker='o'\n",
    "    )\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel(\"% feature integrity change\");\n",
    "plt.axvline(4, linestyle = \"--\", alpha = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Threshold graph explained\n",
    "\n",
    "##### Example \n",
    "\n",
    "At the threshold of 5 missing features, 80.75% of rows are considered intact and will stay in the main data. 19.25% of\n",
    " rows will be set aside. A row can *at most* be missing 5 rows, and therefore could be missing *up to* 6.33% of\n",
    " feature information. \n",
    "\n",
    "##### Definitions\n",
    "\n",
    "**Threshold**: This is the amount of data that is missing from a row. We're trying to find the best threshold at which \n",
    " to keep data in our main dataset, or where to set it aside. Rows AT the threshold will be kept. Rows above the \n",
    " threshold will be set aside. At threshold 50, no data is set aside. At threshold 0, 30% of data is set aside.\n",
    "\n",
    "**\"intact_pct\" and \"incomplete_pct\"**: These are essentially the same thing, just inverted. \n",
    " Intact is what I'm calling rows that make it into the dataset with less missing data. So seeing this number go up means\n",
    " that more rows are considered intact It's actually an indication of lower quality data being included in the main \n",
    " data set. Incomplete rows will go into the data that we're setting aside.\n",
    "\n",
    "**max_feature_loss_pct**: This is the *highest* percent of information that a row will be missing. Many rows will have\n",
    " much more data. The purpose of this line is to give a rudimentary idea of what a worst-case scenario looks like in\n",
    " terms of data missing from rows.\n",
    "\n",
    "##### Chosen threshold: **8** missing features\n",
    "\n",
    "Looking at both the chart and the data, I think 8 is the best threshold. We don't see a huge amount of change in the\n",
    "amount of data changed at larger thresholds, but we introduce a much larger potential for maximum feature loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_by_missing_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rows_by_missing_s` still has a mapping of how many features each row is missing. We can use that with our chosen\n",
    "threshold to provide the right True/False list to azdias to split the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to divide the data into two subsets based on the number of missing\n",
    "# values in each row.\n",
    "threshold = 8 #threshold is INCLUDED in the main data\n",
    "azdias_main = azdias.loc[rows_by_missing_s <= threshold]\n",
    "azdias_missing_data = azdias.loc[rows_by_missing_s > threshold]\n",
    "del azdias #this was pretty big! Now that we've got it separated, lets delete it.\n",
    "del rows_by_missing_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(azdias_main))\n",
    "print(len(azdias_missing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_missing(azdias_main)\n",
    "graph_missing(azdias_missing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of values for at least five columns where there are\n",
    "# no or few missing values, between the two subsets.\n",
    "\n",
    "def compare_normed_main_with_missing(feature_name):\n",
    "    def count_and_norm(s: Series) -> Series:\n",
    "        \"\"\"return the normalized value_counts of a series\"\"\"\n",
    "        vc = s.value_counts()\n",
    "        norm = vc.div(vc.max())\n",
    "        return norm\n",
    "\n",
    "    # Prep corresponding columns from each df\n",
    "    main_s = azdias_main.loc[:,feature_name].astype(float)\n",
    "    main_s.name = f\"{feature_name}_main_  \"\n",
    "    main_s = count_and_norm(main_s)\n",
    "\n",
    "    missing_s = azdias_missing_data.loc[:,feature_name].astype(float)\n",
    "    missing_s.name = f\"{feature_name}_missing_\"\n",
    "    missing_s = count_and_norm(missing_s)\n",
    "\n",
    "    # plotting two series on the same plot\n",
    "    # https://stackoverflow.com/questions/6871201/plot-two-histograms-on-single-chart-with-matplotlib\n",
    "    plt.title(f\"Compare {main_s.name} and {missing_s.name}\")\n",
    "    plt.hist([main_s, missing_s], alpha=0.5, label=[main_s.name, missing_s.name])\n",
    "    \n",
    "    # Fancy legend outside of plot\n",
    "    # https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot\n",
    "    ax = plt.subplot(111)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def random_compare(times_to_compare:int):\n",
    "    # compare_main_with_missing(\"KKK\")\n",
    "    df = azdias_missing_data\n",
    "    # Get, as a series of True/False, columns that have a minimum amount of data\n",
    "    eligible_threshold = 0.8\n",
    "    eligible_to_compare = df.isna().sum()/len(df)<(1 - eligible_threshold)\n",
    "    print(\"count of columns eligible for comparison\",eligible_to_compare.value_counts())\n",
    "\n",
    "    for i in range(times_to_compare):\n",
    "        feature_name = random.choice(eligible_to_compare.index)\n",
    "        compare_normed_main_with_missing(feature_name)\n",
    "        eligible_to_compare.index.drop(feature_name)\n",
    "\n",
    "#I've set the seed in my first cell (random.seed(444)). This should select the same charts every first run.\n",
    "random_compare(5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion 1.1.3: Assess Missing Data in Each Row\n",
    "\n",
    "Comparing the data in a few different columns I'd say the row quality is mixed. Some features in my \"missing\" dataframe\n",
    " seem to closely mimic the distribution of the same columns in my \"intact\" dataframe. Others seem to follow a completely\n",
    " different pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Select and Re-Encode Features\n",
    "\n",
    "Checking for missing data isn't the only way in which you can prepare a dataset for analysis. Since the unsupervised learning techniques to be used will only work on data that is encoded numerically, you need to make a few encoding changes or additional assumptions to be able to make progress. In addition, while almost all of the values in the dataset are encoded using numbers, not all of them represent numeric values. Check the third column of the feature summary (`feat_info`) for a summary of types of measurement.\n",
    "- For numeric and interval data, these features can be kept without changes.\n",
    "- Most of the variables in the dataset are ordinal in nature. While ordinal values may technically be non-linear in spacing, make the simplifying assumption that the ordinal variables can be treated as being interval in nature (that is, kept without any changes).\n",
    "- Special handling may be necessary for the remaining two variable types: categorical, and 'mixed'.\n",
    "\n",
    "In the first two parts of this sub-step, you will perform an investigation of the categorical and mixed-type features and make a decision on each of them, whether you will keep, drop, or re-encode each. Then, in the last part, you will create a new data frame with only the selected and engineered columns.\n",
    "\n",
    "Data wrangling is often the trickiest part of the data analysis process, and there's a lot of it to be done here. But stick with it: once you're done with this step, you'll be ready to get to the machine learning parts of the project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features are there of each data type?\n",
    "\n",
    "#just making sure I can get the right info...\n",
    "#make sure to limit by only columns we kept\n",
    "codex.all_df = codex.all_df.loc[codex.all_df.index.isin(azdias_main.columns)]\n",
    "type_s = codex.all_df.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_s.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2.1: Re-Encode Categorical Features\n",
    "\n",
    "For categorical data, you would ordinarily need to encode the levels as dummy variables. Depending on the number of categories, perform one of the following:\n",
    "- For binary (two-level) categoricals that take numeric values, you can keep them without needing to do anything.\n",
    "- There is one binary variable that takes on non-numeric values. For this one, you need to re-encode the values as numbers or create a dummy variable.\n",
    "- For multi-level categoricals (three or more values), you can choose to encode the values using multiple dummy variables (e.g. via [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)), or (to keep things straightforward) just drop them from the analysis. As always, document your choices in the Discussion section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - About value types (and examples from this data)\n",
    "\n",
    "- Ordinal data - categorical data with naturally ordered categories where it's hard to measure distance between the categories.\n",
    "  - Example:  Likert scale : 1: like, 2: like somewhat, 3: Neutral, 4: Dislike somewhat, 5: Dislike\n",
    "  - From this data ordinal data represents \"on this scale, how sensual is this person\" or \"on this scale, about how crowded is this area\"\n",
    "- Categorical data: data representing categories\n",
    "  - Example: Favorite color? Red, Yellow, or Blue\n",
    "  - In this data, largely used for questions like \"What type of a family are you from\" \"Are you in an environmental group?\" \"Which nationality are you\"\n",
    "- Interval Data - Data points are at regularly spaced, measured, intervals\n",
    "  - Example: Tempurature\n",
    "  - In this data: Not shown above, so I looked in the dictionary - there was only one related feature, and we've cut it out because the column was missing too much data: \"Birthdate of head of household\"\n",
    "- Numeric Data: Standard numbers.\n",
    "  - example: Count of videos in your collection\n",
    "  - In this data: Number of cars in the household, number of adults in the household"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Generator for reviewing data chunks\n",
    "I put together the below python generator and used it in combo with my codex to look through most of the data types. I rapidly edited the generator and used next() calls to print out easy to digest chunks of examples of data, and then cycle to the next chunk. Because of the way I used this feature, and out of a desire to not take up too much space with bulky data sections, I\"m not going to reproduce everything I saw, but I did use it to create the above data summary.\n",
    "\n",
    "In the current state you can also see how I used it to count up categories in categorical datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_display_definition_and_allowed_codes(s:Series, len_limit = None):\n",
    "    display_definition_and_allowed_codes(s.name, len_limit)\n",
    "\n",
    "def give_next_index(df, step = 1):\n",
    "    \"\"\"This will give me definitions and codes of the next step features in this df\n",
    "    REQUIRES: that the index values of the df are feature names.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    start = 0\n",
    "    for i in range(total):\n",
    "        end = start + step\n",
    "        if end > total:\n",
    "            end = total\n",
    "        new_df = df.iloc[start:end]\n",
    "        # display(new_df)\n",
    "        # new_df = new_df.loc[:,[\"definition\",\"type\"]]\n",
    "        new_df = new_df.loc[:,[\"definition\",\"type\",\"allowed_values\"]]\n",
    "        av = new_df.loc[:,\"allowed_values\"]\n",
    "        av = av.apply(lambda x: len(list(x)))\n",
    "        av.name = \"count_allowed\"\n",
    "        new_col_count = av.map(lambda x: x - 1 if x > 2 else 0)\n",
    "        new_col_count.name = \"onehot_extra_cols\"\n",
    "        new_df = pd.concat([new_df,av,new_col_count], axis=1)\n",
    "        display(new_df)\n",
    "        display(sum(new_df[\"onehot_extra_cols\"]))\n",
    "        print(f\"{start+1} - {end} of {total}\")\n",
    "        # new_df.T.apply(s_display_definition_and_allowed_codes)\n",
    "        # print(\":\\n\".join(new_df.index.to_list()))\n",
    "        yield None\n",
    "        start += step\n",
    "\n",
    "codex.all_df = codex.all_df.loc[codex.all_df.index.isin(azdias_main.columns)]\n",
    "categorical_gen = give_next_index(codex.all_df.loc[type_s == \"categorical\"], step=10)\n",
    "next(categorical_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Time to change some data types (Ordinal & Numeric to numeric)\n",
    "I've been holding onto my data as string types. I did that to help me remember that I can't add (for example) eye colors\n",
    " together. After review the instructions and understanding that both ordinal data and numeric data make sense as numbers\n",
    " I'm converting them to a numerical type.\n",
    " \n",
    " I'm still planning to keep categorical and mixed data as strings for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numeric_names = codex.all_df.loc[(type_s == \"ordinal\")|(type_s == \"numeric\")].index\n",
    "azdias_main.loc[:,all_numeric_names] = azdias_main.loc[:,all_numeric_names].apply(pd.to_numeric, errors='raise', downcast=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess categorical variables: which are binary, which are multi-level, and\n",
    "# which one needs to be re-encoded?\n",
    "\n",
    "#closer look at binary\n",
    "binary_df = codex.all_df.loc[type_s == \"categorical\"]\n",
    "binary_df = binary_df.loc[:,[\"definition\",\"allowed_values\"]]\n",
    "\n",
    "#count the allowed values to filter for only binary features\n",
    "av = binary_df.loc[:,\"allowed_values\"]\n",
    "av = av.apply(lambda x: len(list(x)))\n",
    "is_binary = av.map(lambda x: False if x > 2 else True)\n",
    "binary_df = binary_df.loc[is_binary]\n",
    "\n",
    "display(binary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-encode categorical variable(s) to be kept in the analysis.\n",
    "\n",
    "#I want to get a quick view of the missing data in these columns\n",
    "categorical_names = codex.all_df.loc[type_s == \"categorical\"].index\n",
    "graph_missing(azdias_main.loc[:,categorical_names])\n",
    "binary_names = binary_df.index\n",
    "graph_missing(azdias_main.loc[:,binary_names])\n",
    "\n",
    "# #TODO: YOU ARE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that it isn't going to be a big deal to keep our categories and one-hot encode them. \n",
    "\n",
    "Because of that, I'm also not going to worry about fixing any of the binary features. I'm going to let the one-hot\n",
    "encoding in sklearn fix them for me. It wouldn't be hard at all to use a map feature to fix these, but since I'm going\n",
    "to the trouble of one-hot encoding it it would be redundant/wasted effort.\n",
    "\n",
    "Before I can one-hot encode I also have to use an imputer to fix the missing nan values. So for now, I'm going to leave\n",
    "these values alone and finish any other remaining preprocessing. *Then* i'll impute and one-hot encode everything at the\n",
    "same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codex.all_df.loc[type_s == \"numeric\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion 1.2.1: Re-Encode Categorical Features\n",
    "\n",
    "\n",
    "(Double-click this cell and replace this text with your own text, reporting your findings and decisions regarding categorical features. Which ones did you keep, which did you drop, and what engineering steps did you perform?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2.2: Engineer Mixed-Type Features\n",
    "\n",
    "There are a handful of features that are marked as \"mixed\" in the feature summary that require special treatment in order to be included in the analysis. There are two in particular that deserve attention; the handling of the rest are up to your own choices:\n",
    "- \"PRAEGENDE_JUGENDJAHRE\" combines information on three dimensions: generation by decade, movement (mainstream vs. avantgarde), and nation (east vs. west). While there aren't enough levels to disentangle east from west, you should create two new variables to capture the other two dimensions: an interval-type variable for decade, and a binary variable for movement.\n",
    "- \"CAMEO_INTL_2015\" combines information on two axes: wealth and life stage. Break up the two-digit codes by their 'tens'-place and 'ones'-place digits into two new ordinal variables (which, for the purposes of this project, is equivalent to just treating them as their raw numeric values).\n",
    "- If you decide to keep or engineer new features around the other mixed-type features, make sure you note your steps in the Discussion section.\n",
    "\n",
    "Be sure to check `Data_Dictionary.md` for the details needed to finish these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_names = codex.all_df.loc[type_s == \"mixed\"].index\n",
    "\n",
    "# for name in mixed_names:\n",
    "#     codex.display_feature(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I commented out the above because it generated a lot of bloat. Most of the mixed categories have a long list of values.\n",
    "\n",
    "I don't fully understand the data definitions in most of them either. The two clearest mixed categories are the two\n",
    "required in the project instructions, so I think I'll stick with those columns and drop the rest, as it would be a lot\n",
    "of time for little return value for me spend much time on them without additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a drop list and double check it against a \"keep\" list\n",
    "features_to_drop = codex.all_df.loc[type_s == \"mixed\"].index.drop([\"PRAEGENDE_JUGENDJAHRE\",\"CAMEO_INTL_2015\"])\n",
    "print(features_to_drop)\n",
    "\n",
    "features_to_keep = azdias_main.columns.drop(features_to_drop)\n",
    "print(f\"keeping {len(features_to_keep)} features of {len(azdias_main.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_main.drop(columns = features_to_drop, inplace=True)\n",
    "print(len(azdias_main.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default pandas display settings were getting in my way here, so I decided to adjust a bit.\n",
    "print(pd.get_option('display.max_colwidth'))\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate \"PRAEGENDE_JUGENDJAHRE\" and engineer two new variables.\n",
    "display_definition_and_allowed_codes(\"PRAEGENDE_JUGENDJAHRE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to convert each of these numerical codes into a numerical year and a movement.\n",
    "\n",
    "Planned breakdown with 1,2,3 as examples\n",
    "\n",
    "> 1 : 40s - war years (Mainstream, E+W)\n",
    "> \n",
    "> 2\t: 40s - reconstruction years (Avantgarde, E+W)\n",
    "> \n",
    "> 3\t: 50s - economic miracle (Mainstream, E+W)\n",
    "\n",
    "- PRAEGENDE_JUGENDJAHRE_YR (numerical)\n",
    "  - 1 : 40\n",
    "  - 2 : 40\n",
    "  - 3 : 50\n",
    "- PRAEGENDE_JUGENDJAHRE_M (categorical (str))\n",
    "  - Encoding\n",
    "    - 0 = Avantgarde\n",
    "    - 1 = Mainstream\n",
    "  - 1 : 1\n",
    "  - 2 : 0\n",
    "  - 3 : 1\n",
    "\n",
    "##### - But how?\n",
    "1. My awesome little function was able to display an easy piece of text to copy/paste\n",
    "2. Regex can easily break this up... I could also use split functions, but I think regex will be more straightforward in this case\n",
    "3. I'll convert the regex matches into a dataframe and then selectively peel the columns to build a mapping dictionary per feature\n",
    "4. With a mapping feature, all I have to do is map the series! I'll map it twice and save the results to different series.\n",
    "5. Finally, we drop the old feature from the dataframe and concat the two new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. My awesome little function was able to display an easy piece of text to copy/paste\n",
    "\n",
    "pj_text = \"\"\"1\t40s - war years (Mainstream, E+W)\n",
    "2\t40s - reconstruction years (Avantgarde, E+W)\n",
    "3\t50s - economic miracle (Mainstream, E+W)\n",
    "4\t50s - milk bar / Individualisation (Avantgarde, E+W)\n",
    "5\t60s - economic miracle (Mainstream, E+W)\n",
    "6\t60s - generation 68 / student protestors (Avantgarde, W)\n",
    "7\t60s - opponents to the building of the Wall (Avantgarde, E)\n",
    "8\t70s - family orientation (Mainstream, E+W)\n",
    "9\t70s - peace movement (Avantgarde, E+W)\n",
    "10\t80s - Generation Golf (Mainstream, W)\n",
    "11\t80s - ecological awareness (Avantgarde, W)\n",
    "12\t80s - FDJ / communist party youth organisation (Mainstream, E)\n",
    "13\t80s - Swords into ploughshares (Avantgarde, E)\n",
    "14\t90s - digital media kids (Mainstream, E+W)\n",
    "15\t90s - ecological awareness (Avantgarde, E+W)\"\"\"\n",
    "\n",
    "# 2. Regex can easily break this up... I could also use split functions, but I think \n",
    "# regex will be more straightforward in this case\n",
    "\n",
    "pj_pattern =re.compile(r\"(\\d+)\\s+(\\d\\d).*\\((\\w*)\")\n",
    "\n",
    "# 3. I'll convert the regex matches into a dataframe and then selectively peel the \n",
    "# columns to build a mapping dictionary per feature\n",
    "pj_df = pd.DataFrame(pj_pattern.findall(pj_text), columns=[\"code\", \"YR\", \"M\"])\n",
    "pj_df.set_index(\"code\",inplace=True)\n",
    "\n",
    "#Don't forget to convert words to numbers in the categorical feature values!\n",
    "pj_df.loc[:,\"M\"] = pj_df.loc[:,\"M\"].map({\"Avantgarde\": 0, \"Mainstream\": 1})\n",
    "\n",
    "#new values to dictionaries\n",
    "pj_code_to_yr_dict = pj_df.loc[:,[\"YR\"]].to_dict()[\"YR\"]\n",
    "pj_code_to_m_dict = pj_df.loc[:,[\"M\"]].to_dict()[\"M\"]\n",
    "\n",
    "# Stop and check how our mapping dicts look...\n",
    "print(pj_code_to_yr_dict)\n",
    "print(pj_code_to_m_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj_s = azdias_main.loc[:,\"PRAEGENDE_JUGENDJAHRE\"]\n",
    "pj_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. With a mapping feature, all I have to do is map the series! I'll map it twice and \n",
    "# save the results to different series.\n",
    "\n",
    "# I often try to end with \"_s\" or \"_df\" to indicate that the var is a series or a dataframe... \n",
    "# pj_m_s just seemed a bit too cryptic.\n",
    "pj_m_series = pj_s.map(pj_code_to_m_dict)\n",
    "pj_m_series.name = \"PRAEGENDE_JUGENDJAHRE_M\"\n",
    "pj_yr_series = pj_s.map(pj_code_to_yr_dict)\n",
    "pj_yr_series.name = \"PRAEGENDE_JUGENDJAHRE_YR\"\n",
    "\n",
    "#expected transformation, given head() of PRAEGENDE_JUGENDJAHRE\n",
    "\"\"\"\n",
    "Code reference:\n",
    "3\t50s - economic miracle (Mainstream, E+W)\n",
    "8\t70s - family orientation (Mainstream, E+W)\n",
    "14\t90s - digital media kids (Mainstream, E+W)\n",
    "15\t90s - ecological awareness (Avantgarde, E+W)\n",
    "\n",
    "Avantgarde = 0\n",
    "Mainstream = 1\n",
    "\n",
    "Transformed feature:\n",
    "1    14 -> 90, 0\n",
    "2    15 -> 90, 1\n",
    "3     8 -> 70, 0\n",
    "4     8 -> 70, 0\n",
    "5     3 -> 50, 0\n",
    "\"\"\"\n",
    "\n",
    "new_pj = pd.concat([pj_yr_series, pj_m_series], axis=1)\n",
    "\n",
    "display(new_pj.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Finally, we drop the old feature from the dataframe and concat the two new features.\n",
    "\n",
    "# Test first. Doing this all in a one liner\n",
    "test = pd.concat([azdias_main, new_pj], axis=1).drop(columns=\"PRAEGENDE_JUGENDJAHRE\")\n",
    "# check output - should show that test (copy of full database) only contains two cols \n",
    "# containing old col name\n",
    "display(test.loc[:,(test.columns.str.contains(\"PRAEGENDE_JUGENDJAHRE\"))].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#success! persist changes and delete all these big extra DFs\n",
    "azdias_main = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test\n",
    "del new_pj\n",
    "del pj_df\n",
    "del pj_s\n",
    "del pj_m_series\n",
    "del pj_yr_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(azdias_main.loc[:,(azdias_main.columns.str.contains(\"PRAEGENDE_JUGENDJAHRE\"))].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate \"CAMEO_INTL_2015\" and engineer two new variables.\n",
    "display_definition_and_allowed_codes(\"CAMEO_INTL_2015\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature value is pretty easy, it's just that two categorical features have been smashed together. All we have to do is split the two digits.\n",
    "The easy way to do that will be to treat the string as a list and map to the list position of the char we want to keep per new feature.\n",
    "- The 10's place is \"Wealth\" the 1's place is \"life status\"\n",
    "- The new columns will be \"CAMEO_INTL_2015_WEALTH\" and \"CAMEO_INTL_2015_LIFESTATUS\", but since all the feature names are in german lets, be consistant and go with  \n",
    "  - CAMEO_INTL_2015_REICHTUM \n",
    "  - CAMEO_INTL_2015_LEBENSSTATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameo_df = azdias_main.loc[:,\"CAMEO_INTL_2015\"]\n",
    "print(cameo_df.head())\n",
    "#I'm pretty sure that this is a string, but lets just be sure:\n",
    "cameo_r = cameo_df.astype(str).map(lambda x: x[0])\n",
    "cameo_r.name = \"CAMEO_INTL_2015_REICHTUM\"\n",
    "cameo_l = cameo_df.astype(str).map(lambda x: x[1])\n",
    "cameo_l.name = \"CAMEO_INTL_2015_LEBENSSTATUS\"\n",
    "\n",
    "cameo_new = pd.concat([cameo_r, cameo_l], axis=1)\n",
    "display(cameo_new)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great! Lets save and clean up.\n",
    "I'm just going to copy and repurpose the same code I used to save on the previous feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test first. Doing this all in a one liner\n",
    "test = pd.concat([azdias_main, cameo_new], axis=1).drop(columns=\"CAMEO_INTL_2015\")\n",
    "# check output - should show that test (copy of full database) only contains two cols \n",
    "# containing old col name\n",
    "display(test.loc[:,(test.columns.str.contains(\"CAMEO_INTL_2015\"))].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_main = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test\n",
    "del cameo_new\n",
    "del cameo_df\n",
    "del cameo_l\n",
    "del cameo_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion 1.2.2: Engineer Mixed-Type Features\n",
    "\n",
    "(Double-click this cell and replace this text with your own text, reporting your findings and decisions regarding mixed-value features. Which ones did you keep, which did you drop, and what engineering steps did you perform?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2.3: Complete Feature Selection\n",
    "\n",
    "In order to finish this step up, you need to make sure that your data frame now only has the columns that you want to keep. To summarize, the dataframe should consist of the following:\n",
    "- All numeric, interval, and ordinal type columns from the original dataset.\n",
    "- Binary categorical features (all numerically-encoded).\n",
    "- Engineered features from other multi-level categorical features and mixed features.\n",
    "\n",
    "Make sure that for any new columns that you have engineered, that you've excluded the original columns from the final dataset. Otherwise, their values will interfere with the analysis later on the project. For example, you should not keep \"PRAEGENDE_JUGENDJAHRE\", since its values won't be useful for the algorithm: only the values derived from it in the engineered features you created should be retained. As a reminder, your data should only be from **the subset with few or no missing values**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS - Going for the imputer!\n",
    "This is where I\"m going to jump the gun and grab that imputer. I need it so that I can one-hot encode. \n",
    "I just need to remember to include it in my cleaning function.\n",
    "\n",
    "I'm checking out https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html and I think that most of the defaults work well for me\n",
    "Since we're representing both string and numerical data I think I'm going to go with the \"most_frequent\" strategy. It works on both types and I think it's a better idea than filling with a constant that would then have to become an additional one-hot column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_frq = SimpleImputer(strategy=\"most_frequent\")\n",
    "test = imputer_frq.fit_transform(azdias_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test)\n",
    "test.columns = azdias_main.columns\n",
    "graph_missing(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good! Because of the way I designed my graphing/validation/codex stuff, the newly engineered features throw errors. I was trying to be careful to spot surprises when I put them together, here it just creates a little blind spot in my graph.\n",
    "\n",
    "Other than that, we can see that there isn't any missing data. We also see a lot more columns show as \"other_values\" instead of \"valid_values\" - I'm pretty sure that's because of the conversions we made that changed strings to floats. \"1.0\" wouldn't show up in a list of allowed values that included \"1\"\n",
    "\n",
    "I'm going to save this over the top of our main dataframe and move on to onehot encode all of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_main = test\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_s = codex.all_df.type\n",
    "\n",
    "categorical_names = type_s.loc[type_s == \"categorical\"].index\n",
    "#we've dropped some features that still exist in the codex. Lets get a cleaner list of categorical names.\n",
    "categorical_names = azdias_main.columns[azdias_main.columns.isin(categorical_names)].to_list()\n",
    "#We've also added a columns that don't appear in the codex. Let's get them added.\n",
    "categorical_names.extend([\n",
    "    'PRAEGENDE_JUGENDJAHRE_YR',\n",
    "    'PRAEGENDE_JUGENDJAHRE_M',\n",
    "    'CAMEO_INTL_2015_REICHTUM',\n",
    "    'CAMEO_INTL_2015_LEBENSSTATUS'])\n",
    "\n",
    "# azdias_main.loc[:,categorical_names]\n",
    "not_categorical_names = azdias_main.columns[(~azdias_main.columns.isin(categorical_names))]\n",
    "\n",
    "print(\n",
    "    len(categorical_names), \"categorical\\n\",\n",
    "    len(not_categorical_names), \"not categorical\\n\",\n",
    "    len(azdias_main.columns), \"expected total\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've looked at the defaults and they all seem fine for my purposes.\n",
    "# good example of easy use https://stackoverflow.com/questions/71555321/sklearn-preprocessing-onehotencoder-and-the-way-to-read-it\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "one_hot_data = ohe.fit_transform(azdias_main.loc[:,categorical_names]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_categorical = pd.DataFrame(one_hot_data.toarray(), columns=ohe.get_feature_names_out())\n",
    "transformed_categorical.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_main = azdias_main.drop(columns=categorical_names)\n",
    "azdias_main = pd.concat([azdias_main, transformed_categorical], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del transformed_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT : Free life!\n",
    "My kernel crashed a few times while concating these two huge datasets....\n",
    "but I finally did it!\n",
    "\n",
    "I'm building in a save point, just in case. Comment or uncomment these lines at will.\n",
    "\n",
    "HINT TO SELF: Close all your browsers! Free up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_life(mode):\n",
    "    save_location_data = \"data_process_points/azdias_preprocessed_data.csv\"\n",
    "    if mode == \"save\":\n",
    "        azdias_main.to_csv(save_location_data, sep=\";\", index = False)\n",
    "        \n",
    "    if mode == \"load\":\n",
    "        data = pd.read_csv(save_location_data, sep=\";\")\n",
    "        codex = cdd.DataCodex(\n",
    "            data_dict_file=\"data/Data_Dictionary.md\",\n",
    "            feat_summary_file=\"data/AZDIAS_Feature_Summary.csv\",\n",
    "        )\n",
    "        return data, codex\n",
    "\n",
    "# free_life(\"save\")  \n",
    "\n",
    "#remember! to load you have to run the import cell first\n",
    "#make sure free_life(\"save\") is commented out\n",
    "#then run from this cell down\n",
    "# azdias_main, codex = free_life(\"load\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Create a Cleaning Function\n",
    "\n",
    "Even though you've finished cleaning up the general population demographics data, it's important to look ahead to the future and realize that you'll need to perform the same cleaning steps on the customer demographics data. In this substep, complete the function below to execute the main feature selection, encoding, and re-engineering steps you performed above. Then, when it comes to looking at the customer data in Step 3, you can just run this function on that DataFrame to get the trimmed dataset in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(cust_df, imputer_frq=imputer_frq, ohe=ohe):\n",
    "    \"\"\"\n",
    "    Perform feature trimming, re-encoding, and engineering for demographics\n",
    "    data\n",
    "    \n",
    "    INPUT: Demographics DataFrame\n",
    "    OUTPUT: Trimmed and cleaned demographics DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ############## CLEAN NAN\n",
    "\n",
    "    # years\n",
    "    year_df = cust_df.loc[:, [\"GEBURTSJAHR\", \"MIN_GEBAEUDEJAHR\"]]\n",
    "    year_df = year_df.applymap(flag_year)\n",
    "    # numerical\n",
    "    number_s = cust_df.loc[:, \"ANZ_HAUSHALTE_AKTIV\"]\n",
    "    number_s = number_s.map(check_number)\n",
    "\n",
    "    print(\"Completed customer cleaning section: YEARS\")\n",
    "\n",
    "\n",
    "    # for anything else:\n",
    "    # uses: apply_missing_to_nan(feature_name_list:List[str], inspect:int=None)\n",
    "    clean_codex = cdd.DataCodex(\n",
    "        data_dict_file=\"data/Data_Dictionary.md\",\n",
    "        feat_summary_file=\"data/AZDIAS_Feature_Summary.csv\",\n",
    "    ) \n",
    "    quality_df = cust_df.apply(value_validity_sort, cust_codex = clean_codex).T\n",
    "    has_nan_df = quality_df.loc[quality_df[\"nan_codes\"] > 0]\n",
    "    feature_names = has_nan_df.index.to_list()\n",
    "    cust_df.loc[:, feature_names] = apply_missing_to_nan(feature_names, cust_df=cust_df)\n",
    "\n",
    "    ############ NAN DONE\n",
    "    print(\"Completed customer cleaning section: NAN REPLACEMENT\")\n",
    "\n",
    "    ################ REMOVE COLUMNS WITH MISSING DATA\n",
    "\n",
    "    # It seems to me like we can't just remove ANY columns from the customers data\n",
    "    # Doesn't it have to be the same ones? Here's what I removed in the larger set:\n",
    "\n",
    "    drop_features = [\n",
    "        \"TITEL_KZ\",\n",
    "        \"AGER_TYP\",\n",
    "        \"KK_KUNDENTYP\",\n",
    "        \"KBA05_BAUMAX\",\n",
    "        \"GEBURTSJAHR\",\n",
    "        \"ALTER_HH\",\n",
    "    ]\n",
    "    cust_df = cust_df.drop(drop_features, axis=1)\n",
    "    print(\"Completed customer cleaning section: MISSING COLS\")\n",
    "    ############ COLUMNS DONE\n",
    "\n",
    "    ############# REMOVE ROWS\n",
    "\n",
    "    # How much data is missing in each row of the dataset?\n",
    "    rows_by_missing_s = cust_df.T.isna().sum()\n",
    "    threshold = 8 #threshold is INCLUDED in the main data\n",
    "    cust_df_main = cust_df.loc[rows_by_missing_s <= threshold]\n",
    "    # cust_df_missing_data = cust_df.loc[rows_by_missing_s > threshold] # save some space unless we need this...\n",
    "    del cust_df #this was pretty big! Now that we've got it separated, lets delete it.\n",
    "    del rows_by_missing_s\n",
    "    print(\"Completed customer cleaning section: MISSING ROWS\")\n",
    "    ############ ROWS DONE\n",
    "\n",
    "    # change data type for ordinal/numerical\n",
    "\n",
    "    type_s = codex.all_df.loc[codex.all_df.index.isin(cust_df_main.columns)].type\n",
    "    all_numeric_names = codex.all_df.loc[(type_s == \"ordinal\")|(type_s == \"numeric\")].index\n",
    "    cust_df_main.loc[:,all_numeric_names] = cust_df_main.loc[:,all_numeric_names].apply(pd.to_numeric, errors='raise', downcast=None)\n",
    "\n",
    "\n",
    "    # drop additional mixed features\n",
    "\n",
    "    type_s = codex.all_df.loc[codex.all_df.index.isin(cust_df_main.columns)].type\n",
    "    features_to_drop = codex.all_df.loc[type_s == \"mixed\"].index.drop([\"PRAEGENDE_JUGENDJAHRE\",\"CAMEO_INTL_2015\"])\n",
    "    # print(features_to_drop)\n",
    "\n",
    "    features_to_keep = cust_df_main.columns.drop(features_to_drop)\n",
    "    # print(f\"keeping {len(features_to_keep)} features of {len(cust_df_main.columns)}\")\n",
    "\n",
    "    cust_df_main.drop(columns = features_to_drop, inplace=True)\n",
    "    # print(len(cust_df_main.columns))\n",
    "\n",
    "\n",
    "    # ##### engineering 1\n",
    "\n",
    "    # 1. My awesome little function was able to display an easy piece of text to copy/paste\n",
    "\n",
    "    pj_text = \"\"\"1\t40s - war years (Mainstream, E+W)\n",
    "    2\t40s - reconstruction years (Avantgarde, E+W)\n",
    "    3\t50s - economic miracle (Mainstream, E+W)\n",
    "    4\t50s - milk bar / Individualisation (Avantgarde, E+W)\n",
    "    5\t60s - economic miracle (Mainstream, E+W)\n",
    "    6\t60s - generation 68 / student protestors (Avantgarde, W)\n",
    "    7\t60s - opponents to the building of the Wall (Avantgarde, E)\n",
    "    8\t70s - family orientation (Mainstream, E+W)\n",
    "    9\t70s - peace movement (Avantgarde, E+W)\n",
    "    10\t80s - Generation Golf (Mainstream, W)\n",
    "    11\t80s - ecological awareness (Avantgarde, W)\n",
    "    12\t80s - FDJ / communist party youth organisation (Mainstream, E)\n",
    "    13\t80s - Swords into ploughshares (Avantgarde, E)\n",
    "    14\t90s - digital media kids (Mainstream, E+W)\n",
    "    15\t90s - ecological awareness (Avantgarde, E+W)\"\"\"\n",
    "\n",
    "    # 2. Regex can easily break this up... I could also use split functions, but I think\n",
    "    # regex will be more straightforward in this case\n",
    "\n",
    "    pj_pattern =re.compile(r\"(\\d+)\\s+(\\d\\d).*\\((\\w*)\")\n",
    "\n",
    "    # 3. I'll convert the regex matches into a dataframe and then selectively peel the\n",
    "    # columns to build a mapping dictionary per feature\n",
    "    pj_df = pd.DataFrame(pj_pattern.findall(pj_text), columns=[\"code\", \"YR\", \"M\"])\n",
    "    pj_df.set_index(\"code\",inplace=True)\n",
    "\n",
    "    #Don't forget to convert words to numbers in the categorical feature values!\n",
    "    pj_df.loc[:,\"M\"] = pj_df.loc[:,\"M\"].map({\"Avantgarde\": 0, \"Mainstream\": 1})\n",
    "\n",
    "    #new values to dictionaries\n",
    "    pj_code_to_yr_dict = pj_df.loc[:,[\"YR\"]].to_dict()[\"YR\"]\n",
    "    pj_code_to_m_dict = pj_df.loc[:,[\"M\"]].to_dict()[\"M\"]\n",
    "\n",
    "    # Stop and check how our mapping dicts look...\n",
    "    # print(pj_code_to_yr_dict)\n",
    "    # print(pj_code_to_m_dict)\n",
    "\n",
    "    pj_s = cust_df_main.loc[:,\"PRAEGENDE_JUGENDJAHRE\"]\n",
    "    # pj_s.head()\n",
    "\n",
    "    # 4. With a mapping feature, all I have to do is map the series! I'll map it twice and\n",
    "    # save the results to different series.\n",
    "\n",
    "    # I often try to end with \"_s\" or \"_df\" to indicate that the var is a series or a dataframe...\n",
    "    # pj_m_s just seemed a bit too cryptic.\n",
    "    pj_m_series = pj_s.map(pj_code_to_m_dict)\n",
    "    pj_m_series.name = \"PRAEGENDE_JUGENDJAHRE_M\"\n",
    "    pj_yr_series = pj_s.map(pj_code_to_yr_dict)\n",
    "    pj_yr_series.name = \"PRAEGENDE_JUGENDJAHRE_YR\"\n",
    "\n",
    "    #expected transformation, given head() of PRAEGENDE_JUGENDJAHRE\n",
    "    \"\"\"\n",
    "    Code reference:\n",
    "    3\t50s - economic miracle (Mainstream, E+W)\n",
    "    8\t70s - family orientation (Mainstream, E+W)\n",
    "    14\t90s - digital media kids (Mainstream, E+W)\n",
    "    15\t90s - ecological awareness (Avantgarde, E+W)\n",
    "\n",
    "    Avantgarde = 0\n",
    "    Mainstream = 1\n",
    "\n",
    "    Transformed feature:\n",
    "    1    14 -> 90, 0\n",
    "    2    15 -> 90, 1\n",
    "    3     8 -> 70, 0\n",
    "    4     8 -> 70, 0\n",
    "    5     3 -> 50, 0\n",
    "    \"\"\"\n",
    "\n",
    "    new_pj = pd.concat([pj_yr_series, pj_m_series], axis=1)\n",
    "\n",
    "    # 5. Finally, we drop the old feature from the dataframe and concat the two new features.\n",
    "    # Test first. Doing this all in a one liner\n",
    "\n",
    "    test = pd.concat([cust_df_main, new_pj], axis=1).drop(columns=\"PRAEGENDE_JUGENDJAHRE\")\n",
    "    # check output - should show that test (copy of full database) only contains two cols\n",
    "    # containing old col name\n",
    "    # display(test.loc[:,(test.columns.str.contains(\"PRAEGENDE_JUGENDJAHRE\"))].head())\n",
    "    #success! persist changes and delete all these big extra DFs\n",
    "    cust_df_main = test\n",
    "    del test\n",
    "    del new_pj\n",
    "    del pj_df\n",
    "    del pj_s\n",
    "    del pj_m_series\n",
    "    del pj_yr_series\n",
    "\n",
    "    print(\"Completed customer cleaning section: PRAEGENDE_JUGENDJAHRE\")\n",
    "\n",
    "    # engineering 2\n",
    "\n",
    "    cameo_df = cust_df_main.loc[:,\"CAMEO_INTL_2015\"]\n",
    "    # print(cameo_df.head())\n",
    "    #I'm pretty sure that this is a string, but lets just be sure:\n",
    "    cameo_r = cameo_df.astype(str).map(lambda x: x[0])\n",
    "    cameo_r.name = \"CAMEO_INTL_2015_REICHTUM\"\n",
    "    cameo_l = cameo_df.astype(str).map(lambda x: x[1])\n",
    "    cameo_l.name = \"CAMEO_INTL_2015_LEBENSSTATUS\"\n",
    "\n",
    "    cameo_new = pd.concat([cameo_r, cameo_l], axis=1)\n",
    "    # display(cameo_new)\n",
    "    # Test first. Doing this all in a one liner\n",
    "    test = pd.concat([cust_df_main, cameo_new], axis=1).drop(columns=\"CAMEO_INTL_2015\")\n",
    "    # check output - should show that test (copy of full database) only contains two cols\n",
    "    # containing old col name\n",
    "    # display(test.loc[:,(test.columns.str.contains(\"CAMEO_INTL_2015\"))].head())\n",
    "    cust_df_main = test\n",
    "    del test\n",
    "    del cameo_new\n",
    "    del cameo_df\n",
    "    del cameo_l\n",
    "    del cameo_r\n",
    "    print(\"Completed customer cleaning section: CAMEO_INTL_2015\")\n",
    "\n",
    "    # imputer\n",
    "\n",
    "    # imputer_frq = SimpleImputer(strategy=\"most_frequent\")\n",
    "    test = imputer_frq.transform(cust_df_main)\n",
    "    test = pd.DataFrame(test)\n",
    "    test.columns = cust_df_main.columns\n",
    "\n",
    "    graph_missing(test)\n",
    "    cust_df_main = test\n",
    "\n",
    "    del test\n",
    "    print(\"Completed customer cleaning section: IMPUTING MISSING DATA\")\n",
    "\n",
    "    # onehot\n",
    "\n",
    "    type_s = codex.all_df.type\n",
    "\n",
    "    categorical_names = type_s.loc[type_s == \"categorical\"].index\n",
    "    #we've dropped some features that still exist in the codex. Lets get a cleaner list of categorical names.\n",
    "    categorical_names = cust_df_main.columns[cust_df_main.columns.isin(categorical_names)].to_list()\n",
    "    #We've also added a columns that don't appear in the codex. Let's get them added.\n",
    "    categorical_names.extend([\n",
    "        'PRAEGENDE_JUGENDJAHRE_YR',\n",
    "        'PRAEGENDE_JUGENDJAHRE_M',\n",
    "        'CAMEO_INTL_2015_REICHTUM',\n",
    "        'CAMEO_INTL_2015_LEBENSSTATUS'])\n",
    "\n",
    "    # cust_df_main.loc[:,categorical_names]\n",
    "    not_categorical_names = cust_df_main.columns[(~cust_df_main.columns.isin(categorical_names))]\n",
    "\n",
    "    print(\n",
    "        len(categorical_names), \"\\tcategorical\\n\",\n",
    "        len(not_categorical_names), \"\\tnot categorical\\n\",\n",
    "        len(cust_df_main.columns), \"\\texpected total\\n\",\n",
    "    )\n",
    "\n",
    "    one_hot_data = ohe.transform(cust_df_main.loc[:,categorical_names])\n",
    "    transformed_categorical = pd.DataFrame(one_hot_data.toarray(), columns=ohe.get_feature_names_out())\n",
    "    cust_df_main = cust_df_main.drop(columns=categorical_names)\n",
    "    cust_df_main = pd.concat([cust_df_main, transformed_categorical], axis=1)\n",
    "    del transformed_categorical\n",
    "\n",
    "    print(\"Completed customer cleaning section: ONE HOT\")\n",
    "\n",
    "\n",
    "    \n",
    "    # Return the cleaned dataframe.\n",
    "    print(\"ALL CUSTOMER CLEANING FINISHED\")\n",
    "    return cust_df_main\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Transformation\n",
    "\n",
    "### Step 2.1: Apply Feature Scaling\n",
    "\n",
    "Before we apply dimensionality reduction techniques to the data, we need to perform feature scaling so that the principal component vectors are not influenced by the natural differences in scale for features. Starting from this part of the project, you'll want to keep an eye on the [API reference page for sklearn](http://scikit-learn.org/stable/modules/classes.html) to help you navigate to all of the classes and functions that you'll need. In this substep, you'll need to check the following:\n",
    "\n",
    "- sklearn requires that data not have missing values in order for its estimators to work properly. So, before applying the scaler to your data, make sure that you've cleaned the DataFrame of the remaining missing values. This can be as simple as just removing all data points with missing data, or applying an [Imputer](https://scikit-learn.org/0.16/modules/generated/sklearn.preprocessing.Imputer.html) to replace all missing values. You might also try a more complicated procedure where you temporarily remove missing values in order to compute the scaling parameters before re-introducing those missing values and applying imputation. Think about how much missing data you have and what possible effects each approach might have on your analysis, and justify your decision in the discussion section below.\n",
    "- For the actual scaling function, a [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) instance is suggested, scaling each feature to mean 0 and standard deviation 1.\n",
    "- For these classes, you can make use of the `.fit_transform()` method to both fit a procedure to the data as well as apply the transformation to the data at the same time. Don't forget to keep the fit sklearn objects handy, since you'll be applying them to the customer demographics data towards the end of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've not yet cleaned the dataset of all NaN values, then investigate and\n",
    "# do that now.\n",
    "\n",
    "# JLS - Done! We imputed this thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling to the general population demographics data.\n",
    "\n",
    "std_scale = StandardScaler()\n",
    "azdias_main = pd.DataFrame(std_scale.fit_transform(azdias_main), columns=azdias_main.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2.1: Apply Feature Scaling\n",
    "\n",
    "It makes a lot of sense to apply StandardScaler in this case, and so I went with the recommendation. Normalization works when when you're trying to put units with dissimilar units in context with each other. That's not what we're trying to do here - instead we need to format the data to fit a standard statistical shape to make it easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Perform Dimensionality Reduction\n",
    "\n",
    "On your scaled data, you are now ready to apply dimensionality reduction techniques.\n",
    "\n",
    "- Use sklearn's [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) class to apply principal component analysis on the data, thus finding the vectors of maximal variance in the data. To start, you should not set any parameters (so all components are computed) or set a number of components that is at least half the number of features (so there's enough features to see the general trend in variability).\n",
    "- Check out the ratio of variance explained by each principal component as well as the cumulative variance explained. Try plotting the cumulative or sequential values using matplotlib's [`plot()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html) function. Based on what you find, select a value for the number of transformed features you'll retain for the clustering part of the project.\n",
    "- Once you've made a choice for the number of components to keep, make sure you re-fit a PCA instance to perform the decided-on transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JLS Note to self:\n",
    "Remember this awesome layman's breakdown of what PCA is doing:\n",
    "https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "\n",
    "Very very short version - the varience of the data is data that is different enough to help us understand how our \n",
    " entries are different. Features that are pretty much the same for all entries aren't helpful, so as we remove features\n",
    " the super homogenous ones can go. We're looking for an n_components value that helps us maximize the variance while \n",
    " minimizing the number of features.\n",
    "\n",
    " PCA doesn't select features to keep and throw away, instead it does math to combine features mathematically so that the\n",
    " the new cryptic values tell us something useful about how the data is different.\n",
    "\n",
    " e.g. If we're analyzing health in babies, and every baby in our data pool is exactly the same age, then the baby age\n",
    " feature of that data set is useless to helping us understand how the babies are different.\n",
    " PCA doesn't just decide to throw that data away, but the more homogenous features in the set, the fewer components we\n",
    " need out of our PCA outputs to still understand how the rows are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the data.\n",
    "\n",
    "\n",
    "# # We have a ton of columns after one hot, so we're going to take up the instructions on \n",
    "# # the offer to \"set a number of components that is at least half the number of features\"\n",
    "# # instead of not setting any parameters.\n",
    "# n_components = ceil(len(azdias_main)/2)\n",
    "# pca = PCA(n_components)\n",
    "# X_pca = pca.fit_transform(azdias_main)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "_ = pca.fit_transform(azdias_main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder - each component is responsible for X % of variance and it won't change with number of components\n",
    "# so the first component (no mater how many components), will always represent the same amount of variance \n",
    "# The first always represents the most, and the last always represents the least.\n",
    "for n in range(2,5):\n",
    "    print(f\"component variances if n_components = {n}\")\n",
    "    demo_pca = PCA(n)\n",
    "    _ = demo_pca.fit_transform(azdias_main.loc[0:20]) #limiting the features for this reminder_to_self demo\n",
    "    explained_variance_ratio = demo_pca.explained_variance_ratio_\n",
    "    print (\"\\t\",explained_variance_ratio, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the variance accounted for by each principal component.\n",
    "\n",
    "# https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn\n",
    "# getting feature importance\n",
    "\n",
    "\n",
    "#doesn't do anything with feature importance yet.\n",
    "def variance_report(pca: PCA, feature_names) -> Series:\n",
    "    ev = pca.explained_variance_\n",
    "    ev_ratio = pca.explained_variance_ratio_\n",
    "    print(len(ev_ratio))\n",
    "\n",
    "    x_size = pca.n_components_\n",
    "\n",
    "    ev_ratio_sums = []\n",
    "    for n in range(len(ev_ratio)):\n",
    "        ev_ratio_sums.append(sum(ev_ratio[:n+1]))\n",
    "\n",
    "    ev_ratio_sums = pd.Series(ev_ratio_sums)\n",
    "\n",
    "    \"\"\"\n",
    "    reminder to self about meaning of 3 digits in subplot(211)\n",
    "    See \"args\" in https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html\n",
    "    \"\"\"\n",
    "    ax1 = plt.subplot(311)\n",
    "    ax1.plot(ev_ratio)\n",
    "    ax1.title.set_text(\"Explained variance: Component n, by variance ratio\")\n",
    "    plt.show()\n",
    "\n",
    "    ax2 = plt.subplot(312)\n",
    "    ax2.plot(ev_ratio_sums)\n",
    "    ax2.title.set_text(\"Explained variance: n_components by sum of variance ratios\")\n",
    "    ax2.axhline(y=0.95, linestyle=\"dashed\", color=\"orange\")\n",
    "    ax2.axhline(y=0.90, linestyle=\"dashed\", color=\"black\")\n",
    "    ax2.axhline(y=0.85, linestyle=\"dashed\", color=\"violet\")\n",
    "    ax2.axhline(y=0.80, linestyle=\"dashed\", color=\"brown\")\n",
    "    plt.show()\n",
    "\n",
    "    ax3 = plt.subplot(313)\n",
    "    ax3.plot(pd.Series(ev_ratio_sums.loc[(ev_ratio_sums > 0.75)]))\n",
    "    ax3.title.set_text(\"Zoomed sum ratios\")\n",
    "    ax3.axhline(y=0.95, linestyle=\"dashed\", color=\"orange\")\n",
    "    ax3.axhline(y=0.90, linestyle=\"dashed\", color=\"black\")\n",
    "    ax3.axhline(y=0.85, linestyle=\"dashed\", color=\"violet\")\n",
    "    ax3.axhline(y=0.80, linestyle=\"dashed\", color=\"brown\")\n",
    "    plt.show()\n",
    "\n",
    "    return pd.Series(ev_ratio), ev_ratio_sums\n",
    "\n",
    "ev_ratio, ev_ratio_sums = variance_report(pca, azdias_main.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_n_at_ratio(ratio):\n",
    "        n_max = len(ev_ratio_sums)\n",
    "        return round(n_max * ratio, 0)\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.plot(ev_ratio_sums.loc[(ev_ratio_sums > 0.75)])\n",
    "    ax1.title.set_text(\"Zoomed sum ratios\")\n",
    "    ax1.axhline(y=0.95, linestyle=\"dashed\", color=\"orange\")\n",
    "    ax1.axhline(y=0.90, linestyle=\"dashed\", color=\"black\")\n",
    "    ax1.axhline(y=0.85, linestyle=\"dashed\", color=\"violet\")\n",
    "    ax1.axhline(y=0.80, linestyle=\"dashed\", color=\"brown\")\n",
    "    ax1.axvline(x=get_n_at_ratio(0.6), linestyle=\"dashed\", color=\"orange\")\n",
    "    ax1.axvline(x=get_n_at_ratio(0.5), linestyle=\"dashed\", color=\"black\")\n",
    "    ax1.axvline(x=get_n_at_ratio(0.4), linestyle=\"dashed\", color=\"violet\")\n",
    "    ax1.axvline(x=get_n_at_ratio(0.3), linestyle=\"dashed\", color=\"brown\")\n",
    "    plt.show()\n",
    "    \n",
    "    ax2 = plt.subplot(212)\n",
    "    ax2.plot(ev_ratio.loc[(ev_ratio_sums > 0.60)])\n",
    "    ax2.title.set_text(\"Explained variance ratio zoomed\")\n",
    "    ax2.axvline(x=get_n_at_ratio(0.6), linestyle=\"dashed\", color=\"orange\")\n",
    "    ax2.axvline(x=get_n_at_ratio(0.5), linestyle=\"dashed\", color=\"black\")\n",
    "    ax2.axvline(x=get_n_at_ratio(0.4), linestyle=\"dashed\", color=\"violet\")\n",
    "    ax2.axvline(x=get_n_at_ratio(0.3), linestyle=\"dashed\", color=\"brown\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ax1 = plt.subplot(212)\n",
    "    ax1.plot(ev_ratio.loc[(ev_ratio.index > 80)&(ev_ratio.index < 110)])\n",
    "    ax1.title.set_text(\"Explained variance ratio super zoomed\")\n",
    "    ax1.axvline(x=get_n_at_ratio(0.5), linestyle=\"dashed\", color=\"black\")\n",
    "    ax1.axvline(x=get_n_at_ratio(0.4), linestyle=\"dashed\", color=\"violet\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([ev_ratio_sums.iloc[95:101],ev_ratio.iloc[95:101]], index=[\"evr_sums\", \"explained variance ratio\"]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-apply PCA to the data while selecting for number of components to retain.\n",
    "pca = PCA(96)\n",
    "azdias_pca = pca.fit_transform(azdias_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2.2: Perform Dimensionality Reduction\n",
    "\n",
    "By graphing the explained variance ratios, we can see that we get our fastest gains early and then diminishing returns.\n",
    " If we really wanted to limit the feature count we might pic somewhere between 10 and 50 for N, at least enough to get\n",
    " 50% variance maintained.\n",
    "\n",
    "I wanted to hit a higher amount of variance retention, so I'm living with diminishing returns and I'll slice later in\n",
    " the graph. I wanted to get the best bang for my buck, and I also wanted to at least cut off half of my features.\n",
    "\n",
    "Because the rate of change was pretty consistent in the area that I wanted to snip, I'm going to go by a % variance\n",
    "threshold, and choose 85% as the cutoff. That puts me at 96 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Interpret Principal Components\n",
    "\n",
    "Now that we have our transformed principal components, it's a nice idea to check out the weight of each variable on the first few components to see if they can be interpreted in some fashion.\n",
    "\n",
    "As a reminder, each principal component is a unit vector that points in the direction of highest variance (after accounting for the variance captured by earlier principal components). The further a weight is from zero, the more the principal component is in the direction of the corresponding feature. If two features have large weights of the same sign (both positive or both negative), then increases in one tend expect to be associated with increases in the other. To contrast, features with different signs can be expected to show a negative correlation: increases in one variable should result in a decrease in the other.\n",
    "\n",
    "- To investigate the features, you should map each weight to their corresponding feature name, then sort the features according to weight. The most interesting features for each principal component, then, will be those at the beginning and end of the sorted list. Use the data dictionary document to help you understand these most prominent features, their relationships, and what a positive or negative value on the principal component might indicate.\n",
    "- You should investigate and interpret feature associations from the first three principal components in this substep. To help facilitate this, you should write a function that you can call at any time to print the sorted list of feature weights, for the *i*-th principal component. This might come in handy in the next step of the project, when you interpret the tendencies of the discovered clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map weights for the first principal component to corresponding feature names\n",
    "# and then print the linked values, sorted by weight.\n",
    "# HINT: Try defining a function here or in a new cell that you can reuse in the\n",
    "# other cells.\n",
    "\n",
    "\n",
    "def get_component(pca, index):\n",
    "    # pca.components_ : ndarray of shape (n_components, n_features)\n",
    "    return pd.Series(pca.components_[index], index=pca.feature_names_in_).sort_values(ascending=False)\n",
    "\n",
    "def get_component_info(pca, index) -> DataFrame:\n",
    "    c = get_component(pca, index)\n",
    "    c.name = \"weight\"\n",
    "\n",
    "    #get def and translation\n",
    "    def_list = []\n",
    "    trans_list = []\n",
    "    inf_list = []\n",
    "    type_list = []\n",
    "    first_code_list = []\n",
    "    last_code_list = []\n",
    "    count = 0\n",
    "    for feature_name in c.index:\n",
    "        count += 1\n",
    "        found = False\n",
    "        if codex.is_feature_in_data(feature_name) is False:\n",
    "            feature_split = feature_name.rsplit(sep=\"_\", maxsplit=1)\n",
    "            feature_name = feature_split[0]\n",
    "            code = feature_split[1]\n",
    "            if codex.is_feature_in_data(feature_name) is False:\n",
    "                feature_split = feature_name.rsplit(sep=\"_\", maxsplit=1)\n",
    "                def_list.append(\"No definition found\")\n",
    "                trans_list.append(None)\n",
    "                inf_list.append(None)\n",
    "                type_list.append(None)\n",
    "                first_code_list.append(None)\n",
    "                last_code_list.append(None)\n",
    "            else: \n",
    "                found = True\n",
    "        else:\n",
    "            found = True\n",
    "\n",
    "        if found is True:\n",
    "            feature_s = codex.get_feature_as_s(feature_name)\n",
    "            def_list.append(feature_s.loc[\"definition\"])\n",
    "            trans_list.append(feature_s.loc[\"dim_translation\"])\n",
    "            inf_list.append(feature_s.loc[\"information_level\"])\n",
    "            type_list.append(feature_s.loc[\"type\"])\n",
    "            codes = feature_s.loc[\"codes\"]\n",
    "            try:\n",
    "                # Reminder about codes structure:\n",
    "                #     codes_s = pd.Series(definitions, index=symbols, dtype=\"object\") \n",
    "                codes = codes.loc[codes.index.isin(feature_s.loc[\"allowed_values\"])]\n",
    "                first_code_list.append(f\"{codes.index[0]} : {codes.iloc[0]}\") \n",
    "                last_code_list.append(f\"{codes.index[-1]} : {codes.iloc[-1]}\") \n",
    "            except:\n",
    "                first_code_list.append(None)\n",
    "                last_code_list.append(None)\n",
    "                \n",
    "                    \n",
    "            \n",
    "    d = pd.Series(def_list, index=c.index)\n",
    "    d.name = \"definition\"\n",
    "    t = pd.Series(trans_list, index=c.index)\n",
    "    t.name = \"dim_translation\"\n",
    "    inf = pd.Series(inf_list, index=c.index)\n",
    "    inf.name = \"information_level\"\n",
    "    tl = pd.Series(type_list, index=c.index)\n",
    "    tl.name = \"type\"\n",
    "    fc = pd.Series(first_code_list, index=c.index)\n",
    "    fc.name = \"codes[0]\"\n",
    "    lc = pd.Series(last_code_list, index=c.index)\n",
    "    lc.name = \"codes[-1]\"\n",
    "\n",
    "    return pd.DataFrame([inf,c,d,t,fc,lc,tl]).T\n",
    "\n",
    "# get_component(pca, 0)\n",
    "get_component_info(pca, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map weights for the second principal component to corresponding feature names\n",
    "# and then print the linked values, sorted by weight.\n",
    "\n",
    "get_component_info(pca, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map weights for the third principal component to corresponding feature names\n",
    "# and then print the linked values, sorted by weight.\n",
    "\n",
    "get_component_info(pca, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2.3: Interpret Principal Components\n",
    "\n",
    "All three of my strongest components are interesting and easy to interpret\n",
    "\n",
    "1. Region and income \n",
    "   - tough neighborhoods and low wealth bring up this component, the reverse brings it down.\n",
    "2. Idealism and values\n",
    "   - brought up by older generation, a sense of duty, strong religion, frugalness, traditional values, being inconspicuous\n",
    "   - brought down by younger generations, sensuality, event-orientated, standing out\n",
    "   - This stat could qualify for review in the moral/ethical area of ageism because it strongly correlates age with values.\n",
    "3. Personality and Gender\n",
    "   - strongly affected by gender (up male, down female)\n",
    "   - brought up by low affinities for family, society, cooperativeness\n",
    "   - brought down by low affinities for events, dominance, criticality, combativeness\n",
    "   - This stat could require moral/ethical review because it strongly correlates genders with personalities and traditional gender roles.\n",
    "     - It would be important to know how this data was gathered and how it might have been biased.\n",
    "   - This stat could be an interesting study of gender roles and self or societal perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clustering\n",
    "\n",
    "### Step 3.1: Apply Clustering to General Population\n",
    "\n",
    "You've assessed and cleaned the demographics data, then scaled and transformed them. Now, it's time to see how the data clusters in the principal components space. In this substep, you will apply k-means clustering to the dataset and use the average within-cluster distances from each point to their assigned cluster's centroid to decide on a number of clusters to keep.\n",
    "\n",
    "- Use sklearn's [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) class to perform k-means clustering on the PCA-transformed data.\n",
    "- Then, compute the average difference from each point to its assigned cluster's center. **Hint**: The KMeans object's `.score()` method might be useful here, but note that in sklearn, scores tend to be defined so that larger is better. Try applying it to a small, toy dataset, or use an internet search to help your understanding.\n",
    "- Perform the above two steps for a number of different cluster counts. You can then see how the average distance decreases with an increasing number of clusters. However, each additional cluster provides a smaller net benefit. Use this fact to select a final number of clusters in which to group the data. **Warning**: because of the large size of the dataset, it can take a long time for the algorithm to resolve. The more clusters to fit, the longer the algorithm will take. You should test for cluster counts through at least 10 clusters to get the full picture, but you shouldn't need to test for a number of clusters above about 30.\n",
    "- Once you've selected a final number of clusters to use, re-fit a KMeans instance to perform the clustering operation. Make sure that you also obtain the cluster assignments for the general demographics data, since you'll be using them in the final Step 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over a number of different cluster counts...\n",
    "\n",
    "\n",
    "    # run k-means clustering on the data and...\n",
    "    \n",
    "    \n",
    "    # compute the average within-cluster distances.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means implementation\n",
    "def get_score_for_k_clusters(X_pca, k_clusters):\n",
    "    # print(f\"Starting kmeans on {k_clusters} clusters\")\n",
    "    seconds_start = time.time()\n",
    "    k_means = KMeans(k_clusters)\n",
    "    k_model = k_means.fit(X_pca)\n",
    "    # scores are a measure of point distances from centroid\n",
    "    score = k_model.score(X_pca)\n",
    "    seconds_end = time.time()\n",
    "    seconds_to_run = seconds_end - seconds_start\n",
    "    print(f\"\\t{seconds_to_run} seconds to get_scree_data for {k_clusters} clusters. Score = {score}\")\n",
    "    return score\n",
    "\n",
    "#scree plot\n",
    "def get_scree_data(X_pca, k_start, k_stop, k_step):\n",
    "    \"\"\"get data for a scree plot. Remember that k_stop is EXCLUSIVE\"\"\"    \n",
    "    cluster_counts = [k for k in range(k_start, k_stop, k_step)]\n",
    "    if (k_stop-1) not in cluster_counts:\n",
    "        cluster_counts.append((k_stop-1))\n",
    "    print(f\"Preparing to get scree data for these cluster values: {cluster_counts}\")\n",
    "    km_scores = []\n",
    "    cluster_seconds = []\n",
    "    for cluster in cluster_counts:\n",
    "        start_t = time.time()\n",
    "        km_scores.append(get_score_for_k_clusters(X_pca, cluster))\n",
    "        end_t = time.time()\n",
    "        cluster_seconds.append(end_t - start_t)\n",
    "    return cluster_seconds, cluster_counts, km_scores\n",
    "\n",
    "def plot_scree_data(cluster_counts, km_scores):\n",
    "    # plt.plot(cluster_counts, km_scores, linestyle='--', marker='o', color='b')\n",
    "    plt.plot(cluster_counts, km_scores);\n",
    "    plt.xlabel('K clusters used')\n",
    "    plt.ylabel('Score (negative distance from cluster center)')\n",
    "    plt.title('KMeans ScreePlot')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a lot of centers so I want to check on how long these plots take and also limit how many I run at a time.\n",
    "\n",
    "def plot_n_kclusters_in_range(X_pca, n, k_start=None, k_stop=None):\n",
    "    \"\"\"k_stop is EXCLUSIVE to match normal range behavior\"\"\"\n",
    "    component_len = len(X_pca)\n",
    "    if k_start is None:\n",
    "        k_start = 1\n",
    "    if k_stop is None:\n",
    "        k_stop = component_len + 1\n",
    "    #thinking through: k_step = np.floor((k_stop-1-k_start)/n)\n",
    "    # if start = 0, stop = 96 (exclusive), then there are 95 things\n",
    "    # if start = 1, stop = 97 (exclusive), then there are 95 things\n",
    "    # (stop-1) - start = number of things\n",
    "    # floor, because get_scree_data creates a range that includes the\n",
    "    # value before stop if it's not in the range.\n",
    "    if k_start and k_stop:\n",
    "        k_step = int(np.floor((k_stop - 1 - k_start)/n))\n",
    "        if k_step < 1:\n",
    "            k_step = 1\n",
    "\n",
    "\n",
    "        seconds_start = time.time()\n",
    "        cluster_seconds, cluster_counts, km_scores = get_scree_data(X_pca, k_start, k_stop, k_step)\n",
    "        seconds_end = time.time()\n",
    "        seconds_to_run = seconds_end - seconds_start\n",
    "\n",
    "        print(f\"{seconds_to_run} seconds to get_scree_data for \"+\n",
    "            f\"start={k_start}, stop={k_stop}, step={k_step} \")\n",
    "        plot_scree_data(cluster_counts, km_scores)\n",
    "        return cluster_seconds, cluster_counts, km_scores \n",
    "    else:\n",
    "        raise ValueError(f\"start/stop values weren't correctly set: k_start={k_start}, k_stop={k_stop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the change in within-cluster distance across number of clusters.\n",
    "# HINT: Use matplotlib's plot function to visualize this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_seconds, cluster_counts, km_scores = plot_n_kclusters_in_range(\n",
    "    X_pca = azdias_pca, n = 2, k_start=1, k_stop=11)\n",
    "plt.plot([str(t) for t in cluster_counts],cluster_seconds)\n",
    "plt.title(\"Runtime per cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_clusters_by_score =[\n",
    "    [1,-139151727.5074891], \n",
    "    [2,-129593730.50807598], \n",
    "    [3,-125034310.23468415], \n",
    "    [4,-121341293.82486568], \n",
    "    [5,-118991858.3317357], \n",
    "    [6,-117297052.76603276], \n",
    "    [7,-116031017.71899313], \n",
    "    [8,-114923615.85437922], \n",
    "    [9,-113825816.05464804], \n",
    "    [11,-112168430.11480558],\n",
    "    [14,-109513410.66054529],\n",
    "    [17,-107005790.52739587],\n",
    "    [18,-106411005.53301352],\n",
    "    [19,-105906137.64178543],\n",
    "    [21,-103898130.63813205],\n",
    "    [24,-102070727.92574427],\n",
    "    [27,-99697860.50212082],\n",
    "    [29,-98041384.18183574],\n",
    "    ]\n",
    "\n",
    "saved_clusters_by_score = pd.DataFrame(\n",
    "    saved_clusters_by_score, columns=[\"k_clusters\",\"km_score\"])\n",
    "saved_clusters_by_score = saved_clusters_by_score.set_index(\"k_clusters\")\n",
    "\n",
    "plt.plot(saved_clusters_by_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(saved_clusters_by_score.iloc[3:8])\n",
    "plt.xlim(3,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-fit the k-means model with the selected number of clusters and obtain\n",
    "# cluster predictions for the general population demographics data.\n",
    "\n",
    "k_means_6 = KMeans(6)\n",
    "azdias_predict = k_means_6.fit_predict(azdias_pca)\n",
    "del azdias_main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3.1: Apply Clustering to General Population\n",
    "\n",
    "It took a lot of processing power to run kmeans accross so many features, \n",
    " so I ran it in batches and saved the results (similar to the cell with 1-9).\n",
    " I didn't run every point after 1-9. I did generally every 3 until about 30.\n",
    " I created a little overlap to make sure I was covered on each end so the above\n",
    " is just a basic idea.\n",
    "\n",
    "I saved the results of those batches on the side, passed them into a list of lists, and\n",
    " then plotted it to see the whole thing.\n",
    "\n",
    "I'm used \"elbow method\" to narrow in on the point where the data is most likely\n",
    "to have the best clusters and made a best guess at 6 clusters.\n",
    "\n",
    "An alternate to this method would have been to use GausianMixture, which potentially\n",
    "could have been more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Apply All Steps to the Customer Data\n",
    "\n",
    "Now that you have clusters and cluster centers for the general population, it's time to see how the customer data maps on to those clusters. Take care to not confuse this for re-fitting all of the models to the customer data. Instead, you're going to use the fits from the general population to clean, transform, and cluster the customer data. In the last step of the project, you will interpret how the general population fits apply to the customer data.\n",
    "\n",
    "- Don't forget when loading in the customers data, that it is semicolon (`;`) delimited.\n",
    "- Apply the same feature wrangling, selection, and engineering steps to the customer demographics using the `clean_data()` function you created earlier. (You can assume that the customer demographics data has similar meaning behind missing data patterns as the general demographics data.)\n",
    "- Use the sklearn objects from the general demographics data, and apply their transformations to the customers data. That is, you should not be using a `.fit()` or `.fit_transform()` method to re-fit the old objects, nor should you be creating new sklearn objects! Carry the data through the feature scaling, PCA, and clustering steps, obtaining cluster assignments for all of the data in the customer demographics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the customer demographics data.\n",
    "customers_main = pd.read_csv(r\"data\\Udacity_CUSTOMERS_Subset.csv\", sep=\";\")\n",
    "customers_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing, feature transformation, and clustering from the general\n",
    "# demographics onto the customer data, obtaining cluster predictions for the\n",
    "# customer demographics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customers_main = clean_data(customers_main)\n",
    "customers_main.head()\n",
    "\n",
    "# We're going to throw some of my warnings from the codex. Not to worry! That's \n",
    "# just because we engineered some rows that aren't in the codex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_life2(mode):\n",
    "    save_location_data = \"data_process_points/cust_preprocessed_data.csv\"\n",
    "    if mode == \"save\":\n",
    "        customers_main.to_csv(save_location_data, sep=\";\", index = False)\n",
    "        \n",
    "    if mode == \"load\":\n",
    "        data = pd.read_csv(save_location_data, sep=\";\")\n",
    "        codex = cdd.DataCodex(\n",
    "            data_dict_file=\"data/Data_Dictionary.md\",\n",
    "            feat_summary_file=\"data/AZDIAS_Feature_Summary.csv\",\n",
    "        )\n",
    "        return data, codex\n",
    "\n",
    "# free_life2(\"save\")  \n",
    "\n",
    "# The cells below this won't always work without a full run...\n",
    "# PCA and KMeans have to be performed. If the kernel crashes here\n",
    "# we can start again from free_life() for azdias_main... we can't just call\n",
    "# it though, we have to go there and run all cells downward.\n",
    "\n",
    "# This free_life is really to help save us processing time if we overwrite\n",
    "# something important and want to undo.\n",
    "\n",
    "#make sure free_life2(\"save\") is commented out\n",
    "#then run from this cell down\n",
    "# customers_main, codex = free_life2(\"load\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_pca = pca.transform(customers_main)\n",
    "customer_predict = k_means_6.predict(customers_pca)\n",
    "\n",
    "del customer_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Compare Customer Data to Demographics Data\n",
    "\n",
    "At this point, you have clustered data based on demographics of the general population of Germany, and seen how the customer data for a mail-order sales company maps onto those demographic clusters. In this final substep, you will compare the two cluster distributions to see where the strongest customer base for the company is.\n",
    "\n",
    "Consider the proportion of persons in each cluster for the general population, and the proportions for the customers. If we think the company's customer base to be universal, then the cluster assignment proportions should be fairly similar between the two. If there are only particular segments of the population that are interested in the company's products, then we should see a mismatch from one to the other. If there is a higher proportion of persons in a cluster for the customer data compared to the general population (e.g. 5% of persons are assigned to a cluster for the general population, but 15% of the customer data is closest to that cluster's centroid) then that suggests the people in that cluster to be a target audience for the company. On the other hand, the proportion of the data in a cluster being larger in the general population than the customer data (e.g. only 2% of customers closest to a population centroid that captures 6% of the data) suggests that group of persons to be outside of the target demographics.\n",
    "\n",
    "Take a look at the following points in this step:\n",
    "\n",
    "- Compute the proportion of data points in each cluster for the general population and the customer data. Visualizations will be useful here: both for the individual dataset proportions, but also to visualize the ratios in cluster representation between groups. Seaborn's [`countplot()`](https://seaborn.pydata.org/generated/seaborn.countplot.html) or [`barplot()`](https://seaborn.pydata.org/generated/seaborn.barplot.html) function could be handy.\n",
    "  - Recall the analysis you performed in step 1.1.3 of the project, where you separated out certain data points from the dataset if they had more than a specified threshold of missing values. If you found that this group was qualitatively different from the main bulk of the data, you should treat this as an additional data cluster in this analysis. Make sure that you account for the number of data points in this subset, for both the general population and customer datasets, when making your computations!\n",
    "- Which cluster or clusters are overrepresented in the customer dataset compared to the general population? Select at least one such cluster and infer what kind of people might be represented by that cluster. Use the principal component interpretations from step 2.3 or look at additional components to help you make this inference. Alternatively, you can use the `.inverse_transform()` method of the PCA and StandardScaler objects to transform centroids back to the original data space and interpret the retrieved values directly.\n",
    "- Perform a similar investigation for the underrepresented clusters. Which cluster or clusters are underrepresented in the customer dataset compared to the general population, and what kinds of people are typified by these clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the proportion of data in each cluster for the customer data to the\n",
    "# proportion of data in each cluster for the general population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_group_g(g=-1):\n",
    "    # https://stackoverflow.com/questions/49885007/how-to-use-scikit-learn-inverse-transform-with-new-values\n",
    "    # https://www.tutorialspoint.com/how-to-put-a-legend-outside-the-plot-with-pandas\n",
    "    inverse_pca = pca.inverse_transform(k_means_6.cluster_centers_)\n",
    "    group_g = pd.DataFrame(inverse_pca, columns=pca.feature_names_in_)\n",
    "    if g > -1:\n",
    "        relevant_features = get_component(pca, g)\n",
    "        relevant_features = relevant_features.loc[\n",
    "            (relevant_features < relevant_features.quantile(0.02))|\n",
    "            (relevant_features > relevant_features.quantile(0.98))]\n",
    "        group_g = group_g.loc[:,group_g.columns.isin(relevant_features.index)]\n",
    "    group_g = group_g.T.reset_index()\n",
    "    group_g.columns=[\"feature\",\"km_0\",\"km_1\",\"km_2\",\"km_3\",\"km_4\",\"km_5\"]\n",
    "    pd.plotting.parallel_coordinates(group_g,'feature')\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1.0))\n",
    "    title = f\"Top influencers for group km_{g} compared to all groups\"\n",
    "    if g == -1:\n",
    "        title = f\"All features compared to all groups\"\n",
    "        plt.gca().get_legend().remove()       \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions as Categorical\n",
    "\n",
    "pred_c = pd.concat(dict(\n",
    "    customer=pd.Series(customer_predict, name=\"customer\", dtype=\"category\"),\n",
    "    azdias=pd.Series(azdias_predict, name=\"azdias\", dtype=\"category\")\n",
    "    ), axis = 0\n",
    ")\n",
    "pred_c.name=\"km_cluster\"\n",
    "pred_c.index.names=[\"source\", \"point\"]\n",
    "pred_c=pred_c.astype(\"category\")\n",
    "pred_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_by_source = pred_c.groupby(level=\"source\").value_counts()\n",
    "\n",
    "# https://stackoverflow.com/questions/23377108/pandas-percentage-of-total-with-groupby\n",
    "cluster_ratios = clusters_by_source/clusters_by_source.groupby(\"source\").transform('sum')\n",
    "\n",
    "cluster_ratios = cluster_ratios.to_frame().reset_index()\n",
    "cluster_ratios.columns=[\"source\",\"km_cluster\",\"cluster_ratio\"]\n",
    "cluster_ratios = pd.pivot_table(cluster_ratios, values=\"cluster_ratio\", index=\"km_cluster\", columns=[\"source\"])\n",
    "\n",
    "drop_labels = cluster_ratios.loc[cluster_ratios.loc[:,\"customer\"]==0].index\n",
    "comparable_cluster_ratios=cluster_ratios.drop(drop_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.parallel_coordinates(pred_c,'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cluster_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that he customer has a very strong presence in one demographic, an above\n",
    "average presence in one other, and that the general population is more or less evenly\n",
    "divided among the other demographics. To make this easier to look at we'll eliminate\n",
    "all of the rows that don't relate to the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(comparable_cluster_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without a visualization it's easy to see that the customer's demographic is \n",
    "very strongly tied to cluster 2. They have a similar demographic to the population \n",
    "in cluster 4, which makes that cluster worth investigating, but not as strongly as \n",
    "cluster 2.\n",
    "\n",
    "Take a quick look at how the various features map to each group. You'll see that the strongest group 4 influencing features tend to be detractors, or average in many other groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_g()\n",
    "for g in range(6):\n",
    "    plot_group_g(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kinds of people are part of a cluster that is overrepresented in the\n",
    "# customer data compared to the general population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer's highest demographic was group 2, which we analyzed earlier in this notebook.\n",
    "It has strong traditional values and moral codes. It appeals to an older generation and \n",
    "conservative unobtrusive individuals.\n",
    "\n",
    "Based on the groupings I would strongly encourage the company to put a heavy focus on persons\n",
    "who exhibit these features.\n",
    "\n",
    "The secondary demographic that they appealed to was group four, which we haven't analyzed\n",
    "yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_component_info(pca, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 4 appears to be based on family dwellings in established, high earning, multigenerational neighborhoods and living circumstances. The environment described by this group fits nicely with stereotypes surrounding group 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kinds of people are part of a cluster that is underrepresented in the\n",
    "# customer data compared to the general population?\n",
    "\n",
    "get_component_info(pca, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 5 is a good example of a group that doesn't fit this company's target demographic\n",
    "and had the highest representation of groups that weren't represented in the company's\n",
    "customers.\n",
    "\n",
    "Group 5 is (unsurprisingly) inverse to group 4. This is another wealth based grouping\n",
    "a large number of working class and \"socking away\" persons. You'll notice that a large\n",
    "number of these features don't have direct mapping in the data dictionary. It's because\n",
    "we engineered those groups from mixed components. The resulting components were about\n",
    "wealth and life stages, and appear to support the other features that correlate with \n",
    "this group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 3.3: Compare Customer Data to Demographics Data\n",
    "\n",
    "See my above statements for a more detailed review, but the bottom-line takeaway for\n",
    "this company's mailing campaign is that they should focus on older generations\n",
    "with traditional morals, who are well established, in family settings, and in well\n",
    "established neighborhoods.\n",
    "\n",
    "They are less likely to find success with younger generations, crowd seekers,\n",
    "hedonists, and the working class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Congratulations on making it this far in the project! Before you finish, make sure to check through the entire notebook from top to bottom to make sure that your analysis follows a logical flow and all of your findings are documented in **Discussion** cells. Once you've checked over all of your work, you should export the notebook as an HTML document to submit for evaluation. You can do this from the menu, navigating to **File -> Download as -> HTML (.html)**. You will submit both that document and this notebook for your project submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [notebook start](#PreemptiveNotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JLS: Python Development Environment<a name=\"DevEnv\"></a>\n",
    "\n",
    "\n",
    "\n",
    "### Directly installed modules\n",
    "\n",
    "```text\n",
    "jupyter==1.0.0\n",
    "pandas==1.4.3\n",
    "scikit-learn==1.1.1\n",
    "matplotlib==3.5.2\n",
    "seaborn==0.11.2\n",
    "ipython==8.4.0\n",
    "ipykernel==6.15.0\n",
    "```\n",
    "\n",
    "### Full dependency list\n",
    "\n",
    "See also [requirements.txt](requirements.txt)\n",
    "\n",
    "```text\n",
    "argon2-cffi==21.3.0\n",
    "argon2-cffi-bindings==21.2.0\n",
    "asttokens==2.0.5\n",
    "attrs==21.4.0\n",
    "backcall==0.2.0\n",
    "beautifulsoup4==4.11.1\n",
    "bleach==5.0.0\n",
    "cffi==1.15.0\n",
    "cycler==0.11.0\n",
    "debugpy==1.6.0\n",
    "decorator==5.1.1\n",
    "defusedxml==0.7.1\n",
    "entrypoints==0.4\n",
    "executing==0.8.3\n",
    "fastjsonschema==2.15.3\n",
    "fonttools==4.33.3\n",
    "ipykernel==6.15.0\n",
    "ipython==8.4.0\n",
    "ipython-genutils==0.2.0\n",
    "ipywidgets==7.7.1\n",
    "jedi==0.18.1\n",
    "Jinja2==3.1.2\n",
    "joblib==1.1.0\n",
    "jsonschema==4.6.0\n",
    "jupyter==1.0.0\n",
    "jupyter-client==7.3.4\n",
    "jupyter-console==6.4.4\n",
    "jupyter-core==4.10.0\n",
    "jupyterlab-pygments==0.2.2\n",
    "jupyterlab-widgets==1.1.1\n",
    "kiwisolver==1.4.3\n",
    "MarkupSafe==2.1.1\n",
    "matplotlib==3.5.2\n",
    "matplotlib-inline==0.1.3\n",
    "mistune==0.8.4\n",
    "nbclient==0.6.4\n",
    "nbconvert==6.5.0\n",
    "nbformat==5.4.0\n",
    "nest-asyncio==1.5.5\n",
    "notebook==6.4.12\n",
    "nptyping==2.2.0\n",
    "numpy==1.23.0\n",
    "packaging==21.3\n",
    "pandas==1.4.3\n",
    "pandocfilters==1.5.0\n",
    "parso==0.8.3\n",
    "pexpect==4.8.0\n",
    "pickleshare==0.7.5\n",
    "Pillow==9.1.1\n",
    "prometheus-client==0.14.1\n",
    "prompt-toolkit==3.0.29\n",
    "psutil==5.9.1\n",
    "ptyprocess==0.7.0\n",
    "pure-eval==0.2.2\n",
    "pycparser==2.21\n",
    "Pygments==2.12.0\n",
    "pyparsing==3.0.9\n",
    "pyrsistent==0.18.1\n",
    "python-dateutil==2.8.2\n",
    "pytz==2022.1\n",
    "pyzmq==23.2.0\n",
    "qtconsole==5.3.1\n",
    "QtPy==2.1.0\n",
    "scikit-learn==1.1.1\n",
    "scipy==1.8.1\n",
    "seaborn==0.11.2\n",
    "Send2Trash==1.8.0\n",
    "six==1.16.0\n",
    "sklearn==0.0\n",
    "soupsieve==2.3.2.post1\n",
    "stack-data==0.3.0\n",
    "terminado==0.15.0\n",
    "threadpoolctl==3.1.0\n",
    "tinycss2==1.1.1\n",
    "tornado==6.1\n",
    "traitlets==5.3.0\n",
    "wcwidth==0.2.5\n",
    "webencodings==0.5.1\n",
    "widgetsnbextension==3.6.1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310r36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e71c128a88655b58ffe3b6773c2d384ba8479902ab482712a4f2c66ea589250f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
